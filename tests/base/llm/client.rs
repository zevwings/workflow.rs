//! LLM å®¢æˆ·ç«¯æµ‹è¯•
//!
//! æµ‹è¯• LLM å®¢æˆ·ç«¯çš„ JSON è§£æåŠŸèƒ½ï¼Œæ”¯æŒå¤šç§ OpenAI å…¼å®¹æ ¼å¼ã€‚
//!
//! ## æµ‹è¯•ç­–ç•¥
//!
//! - æ‰€æœ‰æµ‹è¯•è¿”å› `Result<()>`ï¼Œä½¿ç”¨ `?` è¿ç®—ç¬¦å¤„ç†é”™è¯¯
//! - æµ‹è¯•å¤šç§ OpenAI å…¼å®¹çš„ JSON æ ¼å¼
//! - ä½¿ç”¨å¿«ç…§æµ‹è¯•éªŒè¯ JSON ç»“æ„

use color_eyre::Result;
use insta::assert_json_snapshot;
use pretty_assertions::assert_eq;

use serde_json::json;
use workflow::base::llm::client::LLMClient;

#[test]
fn test_extract_from_openai_standard() -> Result<()> {
    // æµ‹è¯•æ ‡å‡† OpenAI æ ¼å¼ï¼ˆåŒ…å«æ‰€æœ‰å¿…éœ€å­—æ®µï¼‰
    let json = json!({
        "id": "chatcmpl-test",
        "object": "chat.completion",
        "created": 1234567890,
        "model": "gpt-3.5-turbo",
        "choices": [{
            "index": 0,
            "message": {
                "role": "assistant",
                "content": "Test content"
            },
            "finish_reason": "stop"
        }],
        "usage": {
            "prompt_tokens": 10,
            "completion_tokens": 5,
            "total_tokens": 15
        }
    });

    let client = LLMClient::global();
    let result = client.extract_content(&json)?;
    assert_eq!(result, "Test content");

    // ä½¿ç”¨å¿«ç…§æµ‹è¯•éªŒè¯ JSON ç»“æ„
    assert_json_snapshot!("openai_standard_response", json);
    Ok(())
}

#[test]
fn test_extract_from_openai_proxy() -> Result<()> {
    // æµ‹è¯• proxy æ ¼å¼ï¼ˆåŒ…å«æ‰©å±•å­—æ®µï¼Œä½†ç¬¦åˆ OpenAI æ ‡å‡†ï¼‰
    let json = json!({
        "id": "chatcmpl-CfonRS9pFvyJW33Opwz83wHhVIGnz",
        "object": "chat.completion",
        "created": 1764082745,
        "model": "gpt-3.5-turbo-0125",
        "choices": [{
            "index": 0,
            "message": {
                "role": "assistant",
                "content": "Test response content",
                "refusal": null,
                "annotations": []
            },
            "logprobs": null,
            "finish_reason": "stop"
        }],
        "usage": {
            "prompt_tokens": 62,
            "completion_tokens": 56,
            "total_tokens": 118,
            "prompt_tokens_details": {
                "cached_tokens": 0,
                "audio_tokens": 0
            },
            "completion_tokens_details": {
                "reasoning_tokens": 0,
                "audio_tokens": 0,
                "accepted_prediction_tokens": 0,
                "rejected_prediction_tokens": 0
            }
        },
        "service_tier": "default",
        "system_fingerprint": null
    });

    let client = LLMClient::global();
    let result = client.extract_content(&json)?;
    assert_eq!(result, "Test response content");

    // ä½¿ç”¨å¿«ç…§æµ‹è¯•éªŒè¯ JSON ç»“æ„
    assert_json_snapshot!("openai_proxy_response", json);
    Ok(())
}

#[test]
fn test_extract_from_cerebras_proxy() -> Result<()> {
    // æµ‹è¯•å¦ä¸€ç§ proxy æ ¼å¼å˜ä½“ï¼ˆå­—æ®µé¡ºåºä¸åŒï¼Œç¼ºå°‘éƒ¨åˆ†æ‰©å±•å­—æ®µï¼Œä½†æœ‰æ–°çš„ time_infoï¼‰
    let json = json!({
        "id": "chatcmpl-97c1fe15-05df-490d-a1b9-8540771db334",
        "choices": [{
            "finish_reason": "stop",
            "index": 0,
            "message": {
                "content": "Test response content",
                "role": "assistant"
            }
        }],
        "created": 1764083329,
        "model": "qwen-3-235b-a22b-instruct-2507",
        "system_fingerprint": "fp_d2d9b827ee854c39818d",
        "object": "chat.completion",
        "usage": {
            "total_tokens": 186,
            "completion_tokens": 123,
            "prompt_tokens": 63
        },
        "time_info": {
            "queue_time": 0.001855552,
            "prompt_time": 0.003562439,
            "completion_time": 0.120426404,
            "total_time": 0.12685751914978027,
            "created": 1764083329.0582647
        }
    });

    let client = LLMClient::global();
    let result = client.extract_content(&json)?;
    assert_eq!(result, "Test response content");

    // ä½¿ç”¨å¿«ç…§æµ‹è¯•éªŒè¯ JSON ç»“æ„
    assert_json_snapshot!("cerebras_proxy_response", json);
    Ok(())
}

// ==================== LLMClient æ–¹æ³•æµ‹è¯• ====================

#[test]
fn test_llm_client_global() {
    // æµ‹è¯• LLMClient::global() æ–¹æ³•ï¼ˆè¦†ç›– client.rs:59-62ï¼‰
    let client1 = LLMClient::global();
    let client2 = LLMClient::global();

    // éªŒè¯è¿”å›çš„æ˜¯åŒä¸€ä¸ªå®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
    assert!(std::ptr::eq(client1, client2));
}

#[test]
fn test_extract_content_empty_choices() {
    // æµ‹è¯• extract_content() æ–¹æ³• - ç©º choices æ•°ç»„ï¼ˆè¦†ç›– client.rs:228-244ï¼‰
    let json = json!({
        "id": "test",
        "object": "chat.completion",
        "created": 1234567890,
        "model": "gpt-3.5-turbo",
        "choices": [],
        "usage": {
            "prompt_tokens": 10,
            "completion_tokens": 5,
            "total_tokens": 15
        }
    });

    let client = LLMClient::global();
    let result = client.extract_content(&json);

    // ç©º choices åº”è¯¥è¿”å›é”™è¯¯
    assert!(result.is_err());
    if let Err(e) = result {
        assert!(e.to_string().contains("No content in response"));
    }
}

#[test]
fn test_extract_content_null_content() {
    // æµ‹è¯• extract_content() æ–¹æ³• - content ä¸º nullï¼ˆè¦†ç›– client.rs:228-244ï¼‰
    let json = json!({
        "id": "test",
        "object": "chat.completion",
        "created": 1234567890,
        "model": "gpt-3.5-turbo",
        "choices": [{
            "index": 0,
            "message": {
                "role": "assistant",
                "content": null
            },
            "finish_reason": "stop"
        }],
        "usage": {
            "prompt_tokens": 10,
            "completion_tokens": 5,
            "total_tokens": 15
        }
    });

    let client = LLMClient::global();
    let result = client.extract_content(&json);

    // content ä¸º null åº”è¯¥è¿”å›é”™è¯¯
    assert!(result.is_err());
    if let Err(e) = result {
        assert!(e.to_string().contains("No content in response"));
    }
}

#[test]
fn test_extract_content_whitespace_trimming() -> Result<()> {
    // æµ‹è¯• extract_content() æ–¹æ³• - å†…å®¹é¦–å°¾ç©ºç™½è¢«ä¿®å‰ªï¼ˆè¦†ç›– client.rs:243ï¼‰
    let json = json!({
        "id": "test",
        "object": "chat.completion",
        "created": 1234567890,
        "model": "gpt-3.5-turbo",
        "choices": [{
            "index": 0,
            "message": {
                "role": "assistant",
                "content": "  \n  Test content with whitespace  \n  "
            },
            "finish_reason": "stop"
        }],
        "usage": {
            "prompt_tokens": 10,
            "completion_tokens": 5,
            "total_tokens": 15
        }
    });

    let client = LLMClient::global();
    let result = client.extract_content(&json)?;

    // éªŒè¯é¦–å°¾ç©ºç™½è¢«ä¿®å‰ª
    assert_eq!(result, "Test content with whitespace");
    assert!(!result.starts_with(' '));
    assert!(!result.ends_with(' '));
    Ok(())
}

#[test]
fn test_extract_content_multiple_choices() -> Result<()> {
    // æµ‹è¯• extract_content() æ–¹æ³• - å¤šä¸ª choicesï¼Œå–ç¬¬ä¸€ä¸ªï¼ˆè¦†ç›– client.rs:237-240ï¼‰
    let json = json!({
        "id": "test",
        "object": "chat.completion",
        "created": 1234567890,
        "model": "gpt-3.5-turbo",
        "choices": [
            {
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": "First choice"
                },
                "finish_reason": "stop"
            },
            {
                "index": 1,
                "message": {
                    "role": "assistant",
                    "content": "Second choice"
                },
                "finish_reason": "stop"
            }
        ],
        "usage": {
            "prompt_tokens": 10,
            "completion_tokens": 5,
            "total_tokens": 15
        }
    });

    let client = LLMClient::global();
    let result = client.extract_content(&json)?;

    // åº”è¯¥è¿”å›ç¬¬ä¸€ä¸ª choice çš„å†…å®¹
    assert_eq!(result, "First choice");
    Ok(())
}

#[test]
fn test_extract_content_invalid_json_structure() {
    // æµ‹è¯• extract_content() æ–¹æ³• - æ— æ•ˆçš„ JSON ç»“æ„ï¼ˆè¦†ç›– client.rs:228-244ï¼‰
    let json = json!({
        "id": "test",
        "invalid_structure": true
    });

    let client = LLMClient::global();
    let result = client.extract_content(&json);

    // æ— æ•ˆç»“æ„åº”è¯¥è¿”å›é”™è¯¯
    assert!(result.is_err());
}

#[test]
fn test_extract_content_missing_required_fields() {
    // æµ‹è¯• extract_content() æ–¹æ³• - ç¼ºå°‘å¿…éœ€å­—æ®µï¼ˆè¦†ç›– client.rs:228-244ï¼‰
    let json = json!({
        "id": "test"
    });

    let client = LLMClient::global();
    let result = client.extract_content(&json);

    // ç¼ºå°‘å¿…éœ€å­—æ®µåº”è¯¥è¿”å›é”™è¯¯
    assert!(result.is_err());
}

#[test]
fn test_extract_content_with_finish_reason_length() -> Result<()> {
    // æµ‹è¯• extract_content() æ–¹æ³• - finish_reason ä¸º lengthï¼ˆè¦†ç›– client.rs:228-244ï¼‰
    let json = json!({
        "id": "test",
        "object": "chat.completion",
        "created": 1234567890,
        "model": "gpt-3.5-turbo",
        "choices": [{
            "index": 0,
            "message": {
                "role": "assistant",
                "content": "Partial content"
            },
            "finish_reason": "length"
        }],
        "usage": {
            "prompt_tokens": 10,
            "completion_tokens": 5,
            "total_tokens": 15
        }
    });

    let client = LLMClient::global();
    let result = client.extract_content(&json)?;

    // finish_reason ä¸º length æ—¶ä¹Ÿåº”è¯¥èƒ½æå–å†…å®¹
    assert_eq!(result, "Partial content");
    Ok(())
}

#[test]
fn test_extract_content_with_finish_reason_stop() -> Result<()> {
    // æµ‹è¯• extract_content() æ–¹æ³• - finish_reason ä¸º stopï¼ˆè¦†ç›– client.rs:228-244ï¼‰
    let json = json!({
        "id": "test",
        "object": "chat.completion",
        "created": 1234567890,
        "model": "gpt-3.5-turbo",
        "choices": [{
            "index": 0,
            "message": {
                "role": "assistant",
                "content": "Complete content"
            },
            "finish_reason": "stop"
        }],
        "usage": {
            "prompt_tokens": 10,
            "completion_tokens": 5,
            "total_tokens": 15
        }
    });

    let client = LLMClient::global();
    let result = client.extract_content(&json)?;

    // finish_reason ä¸º stop æ—¶åº”è¯¥èƒ½æå–å†…å®¹
    assert_eq!(result, "Complete content");
    Ok(())
}

// ==================== LLMClient æ„å»ºæ–¹æ³•æµ‹è¯•ï¼ˆé€šè¿‡ call é—´æ¥æµ‹è¯•ï¼‰====================
// æ³¨æ„ï¼šè¿™äº›æµ‹è¯•éœ€è¦å®é™…çš„é…ç½®æ–‡ä»¶ï¼Œä½†ä¼šæµ‹è¯• build_url, build_headers, build_model, build_payload ç­‰æ–¹æ³•

/// æµ‹è¯•LLMå®¢æˆ·ç«¯ä¸OpenAI providerçš„å®é™…APIè°ƒç”¨
///
/// ## æµ‹è¯•ç›®çš„
/// éªŒè¯`LLMClient`èƒ½å¤ŸæˆåŠŸè°ƒç”¨çœŸå®çš„OpenAI APIå¹¶æ­£ç¡®å¤„ç†å“åº”ã€‚
/// è¦†ç›–æºä»£ç : `client.rs:77-134`, `build_url:148`, `build_model:189-190`
///
/// ## ä¸ºä»€ä¹ˆè¢«å¿½ç•¥
/// - **éœ€è¦ç½‘ç»œè¿æ¥**: éœ€è¦å®é™…è¿æ¥åˆ°OpenAI APIæœåŠ¡å™¨
/// - **éœ€è¦APIå¯†é’¥**: éœ€è¦æœ‰æ•ˆçš„OpenAI API keyé…ç½®åœ¨configæ–‡ä»¶ä¸­
/// - **äº§ç”Ÿè´¹ç”¨**: æ¯æ¬¡APIè°ƒç”¨ä¼šäº§ç”Ÿå®é™…è´¹ç”¨ï¼ˆçº¦$0.001-0.01ï¼Œå–å†³äºæ¨¡å‹å’Œtokensï¼‰
/// - **ä¸ç¨³å®šæ€§**: ç½‘ç»œé—®é¢˜ã€APIé™æµã€æœåŠ¡ä¸­æ–­å¯èƒ½å¯¼è‡´æµ‹è¯•å¤±è´¥
/// - **CIä¸é€‚ç”¨**: CIç¯å¢ƒé€šå¸¸æ²¡æœ‰APIå¯†é’¥ä¸”ä¸åº”äº§ç”Ÿè´¹ç”¨
///
/// ## å¦‚ä½•æ‰‹åŠ¨è¿è¡Œ
/// ```bash
/// # 1. ç¡®ä¿å·²é…ç½®OpenAI API key
/// # åœ¨ ~/.workflow/config/workflow.toml ä¸­:
/// # [llm]
/// # provider = "OpenAI"
/// # api_key = "sk-..."
///
/// # 2. è¿è¡Œæµ‹è¯•
/// cargo test test_llm_client_call_with_openai_provider -- --ignored --nocapture
/// ```
/// **ğŸ’° æ³¨æ„**: æ­¤æµ‹è¯•ä¼šäº§ç”Ÿå®é™…çš„APIè°ƒç”¨è´¹ç”¨ï¼ä½¿ç”¨gpt-3.5-turboæ¨¡å‹ï¼Œçº¦$0.001-0.01/æ¬¡
///
/// ## æµ‹è¯•åœºæ™¯
/// 1. è·å–LLMå®¢æˆ·ç«¯å•ä¾‹
/// 2. æ„é€ è¯·æ±‚å‚æ•°ï¼ˆsystem prompt, user prompt, tokensç­‰ï¼‰
/// 3. è°ƒç”¨OpenAI APIï¼ˆgpt-3.5-turboæ¨¡å‹ï¼‰
/// 4. ç­‰å¾…APIå“åº”
/// 5. éªŒè¯å“åº”æ ¼å¼å’Œå†…å®¹
///
/// ## é¢„æœŸè¡Œä¸º
/// - æˆåŠŸæƒ…å†µï¼šè¿”å›`Ok(LLMResponse)`åŒ…å«AIç”Ÿæˆçš„å›å¤
/// - å¤±è´¥æƒ…å†µï¼šè¿”å›`Err(...)`å¹¶åŒ…å«æ¸…æ™°çš„é”™è¯¯ä¿¡æ¯ï¼š
///   - API keyæ— æ•ˆæˆ–ç¼ºå¤±
///   - ç½‘ç»œè¿æ¥é”™è¯¯
///   - APIé™æµï¼ˆrate limitï¼‰
///   - æ¨¡å‹ä¸å­˜åœ¨æˆ–æ— æƒé™
/// - å“åº”å†…å®¹ç¬¦åˆè¯·æ±‚çš„max_tokensé™åˆ¶
/// - æ­£ç¡®å¤„ç†APIçš„å„ç§é”™è¯¯ç 
#[test]
#[ignore] // éœ€è¦ç½‘ç»œè¯·æ±‚ï¼Œé»˜è®¤å¿½ç•¥
fn test_llm_client_call_with_openai_provider() {
    // æµ‹è¯• call() æ–¹æ³• - OpenAI providerï¼ˆè¦†ç›– client.rs:77-134, build_url:148, build_model:189-190ï¼‰
    // æ³¨æ„ï¼šè¿™ä¸ªæµ‹è¯•éœ€è¦æœ‰æ•ˆçš„ OpenAI API key å’Œç½‘ç»œè¿æ¥
    use workflow::base::llm::types::LLMRequestParams;

    let client = LLMClient::global();
    let params = LLMRequestParams {
        system_prompt: "You are a helpful assistant.".to_string(),
        user_prompt: "Say hello".to_string(),
        max_tokens: Some(10),
        temperature: 0.5,
        model: "gpt-3.5-turbo".to_string(),
    };

    // è¿™ä¸ªæµ‹è¯•éœ€è¦å®é™…çš„ API keyï¼Œæ‰€ä»¥é»˜è®¤å¿½ç•¥
    let result = client.call(&params);
    assert!(result.is_ok() || result.is_err()); // å¯èƒ½æˆåŠŸæˆ–å¤±è´¥ï¼Œå–å†³äºé…ç½®
}

/// æµ‹è¯•LLMå®¢æˆ·ç«¯ä¸DeepSeek providerçš„å®é™…APIè°ƒç”¨
///
/// ## æµ‹è¯•ç›®çš„
/// éªŒè¯`LLMClient`èƒ½å¤ŸæˆåŠŸè°ƒç”¨çœŸå®çš„DeepSeek APIå¹¶æ­£ç¡®å¤„ç†å“åº”ã€‚
/// è¦†ç›–æºä»£ç : `client.rs:149`, `build_model:189-190`
///
/// ## ä¸ºä»€ä¹ˆè¢«å¿½ç•¥
/// - **éœ€è¦ç½‘ç»œè¿æ¥**: éœ€è¦å®é™…è¿æ¥åˆ°DeepSeek APIæœåŠ¡å™¨
/// - **éœ€è¦APIå¯†é’¥**: éœ€è¦æœ‰æ•ˆçš„DeepSeek API keyé…ç½®åœ¨configæ–‡ä»¶ä¸­
/// - **äº§ç”Ÿè´¹ç”¨**: æ¯æ¬¡APIè°ƒç”¨ä¼šäº§ç”Ÿå®é™…è´¹ç”¨ï¼ˆçº¦$0.0005-0.005ï¼Œå–å†³äºæ¨¡å‹å’Œtokensï¼‰
/// - **ä¸ç¨³å®šæ€§**: ç½‘ç»œé—®é¢˜ã€APIé™æµã€æœåŠ¡ä¸­æ–­å¯èƒ½å¯¼è‡´æµ‹è¯•å¤±è´¥
/// - **CIä¸é€‚ç”¨**: CIç¯å¢ƒé€šå¸¸æ²¡æœ‰APIå¯†é’¥ä¸”ä¸åº”äº§ç”Ÿè´¹ç”¨
///
/// ## å¦‚ä½•æ‰‹åŠ¨è¿è¡Œ
/// ```bash
/// # 1. ç¡®ä¿å·²é…ç½®DeepSeek API key
/// # åœ¨ ~/.workflow/config/workflow.toml ä¸­:
/// # [llm]
/// # provider = "DeepSeek"
/// # api_key = "sk-..."
///
/// # 2. è¿è¡Œæµ‹è¯•
/// cargo test test_llm_client_call_with_deepseek_provider -- --ignored --nocapture
/// ```
/// **ğŸ’° æ³¨æ„**: æ­¤æµ‹è¯•ä¼šäº§ç”Ÿå®é™…çš„APIè°ƒç”¨è´¹ç”¨ï¼ä½¿ç”¨deepseek-chatæ¨¡å‹ï¼Œçº¦$0.0005-0.005/æ¬¡
///
/// ## æµ‹è¯•åœºæ™¯
/// 1. è·å–LLMå®¢æˆ·ç«¯å•ä¾‹
/// 2. æ„é€ è¯·æ±‚å‚æ•°ï¼ˆsystem prompt, user prompt, tokensç­‰ï¼‰
/// 3. è°ƒç”¨DeepSeek APIï¼ˆdeepseek-chatæ¨¡å‹ï¼‰
/// 4. ç­‰å¾…APIå“åº”
/// 5. éªŒè¯å“åº”æ ¼å¼å’Œå†…å®¹
///
/// ## é¢„æœŸè¡Œä¸º
/// - æˆåŠŸæƒ…å†µï¼šè¿”å›`Ok(LLMResponse)`åŒ…å«AIç”Ÿæˆçš„å›å¤
/// - å¤±è´¥æƒ…å†µï¼šè¿”å›`Err(...)`å¹¶åŒ…å«æ¸…æ™°çš„é”™è¯¯ä¿¡æ¯ï¼š
///   - API keyæ— æ•ˆæˆ–ç¼ºå¤±
///   - ç½‘ç»œè¿æ¥é”™è¯¯
///   - APIé™æµï¼ˆrate limitï¼‰
///   - æ¨¡å‹ä¸å­˜åœ¨æˆ–æ— æƒé™
/// - å“åº”å†…å®¹ç¬¦åˆè¯·æ±‚çš„max_tokensé™åˆ¶
/// - æ­£ç¡®å¤„ç†DeepSeekç‰¹å®šçš„APIæ ¼å¼å’Œé”™è¯¯ç 
#[test]
#[ignore] // éœ€è¦ç½‘ç»œè¯·æ±‚ï¼Œé»˜è®¤å¿½ç•¥
fn test_llm_client_call_with_deepseek_provider() {
    // æµ‹è¯• call() æ–¹æ³• - DeepSeek providerï¼ˆè¦†ç›– client.rs:149, build_model:189-190ï¼‰
    use workflow::base::llm::types::LLMRequestParams;

    let client = LLMClient::global();
    let params = LLMRequestParams {
        system_prompt: "You are a helpful assistant.".to_string(),
        user_prompt: "Say hello".to_string(),
        max_tokens: Some(10),
        temperature: 0.5,
        model: "deepseek-chat".to_string(),
    };

    // è¿™ä¸ªæµ‹è¯•éœ€è¦å®é™…çš„ API keyï¼Œæ‰€ä»¥é»˜è®¤å¿½ç•¥
    let result = client.call(&params);
    assert!(result.is_ok() || result.is_err()); // å¯èƒ½æˆåŠŸæˆ–å¤±è´¥ï¼Œå–å†³äºé…ç½®
}

/// æµ‹è¯•LLMå®¢æˆ·ç«¯ä¸Proxy providerçš„å®é™…APIè°ƒç”¨
///
/// ## æµ‹è¯•ç›®çš„
/// éªŒè¯`LLMClient`èƒ½å¤Ÿé€šè¿‡è‡ªå®šä¹‰ä»£ç†URLè°ƒç”¨LLM APIå¹¶æ­£ç¡®å¤„ç†å“åº”ã€‚
/// è¦†ç›–æºä»£ç : `client.rs:150-156`, `build_model:192`
///
/// ## ä¸ºä»€ä¹ˆè¢«å¿½ç•¥
/// - **éœ€è¦ç½‘ç»œè¿æ¥**: éœ€è¦å®é™…è¿æ¥åˆ°ä»£ç†æœåŠ¡å™¨
/// - **éœ€è¦ä»£ç†é…ç½®**: éœ€è¦æœ‰æ•ˆçš„proxy URLå’ŒAPI keyé…ç½®åœ¨configæ–‡ä»¶ä¸­
/// - **äº§ç”Ÿè´¹ç”¨**: æ¯æ¬¡APIè°ƒç”¨å¯èƒ½äº§ç”Ÿè´¹ç”¨ï¼ˆå–å†³äºä»£ç†æœåŠ¡çš„è®¡è´¹æ–¹å¼ï¼‰
/// - **ç¯å¢ƒä¾èµ–**: éœ€è¦å¯ç”¨çš„ä»£ç†æœåŠ¡å™¨
/// - **ä¸ç¨³å®šæ€§**: ç½‘ç»œé—®é¢˜ã€ä»£ç†æœåŠ¡ä¸­æ–­å¯èƒ½å¯¼è‡´æµ‹è¯•å¤±è´¥
/// - **CIä¸é€‚ç”¨**: CIç¯å¢ƒé€šå¸¸æ²¡æœ‰ä»£ç†é…ç½®ä¸”ä¸åº”äº§ç”Ÿè´¹ç”¨
///
/// ## å¦‚ä½•æ‰‹åŠ¨è¿è¡Œ
/// ```bash
/// # 1. ç¡®ä¿å·²é…ç½®Proxy
/// # åœ¨ ~/.workflow/config/workflow.toml ä¸­:
/// # [llm]
/// # provider = "Proxy"
/// # proxy_url = "https://your-proxy.com/v1"
/// # api_key = "your-key"
///
/// # 2. è¿è¡Œæµ‹è¯•
/// cargo test test_llm_client_call_with_proxy_provider -- --ignored --nocapture
/// ```
/// **ğŸ’° æ³¨æ„**: æ­¤æµ‹è¯•å¯èƒ½äº§ç”ŸAPIè°ƒç”¨è´¹ç”¨ï¼è´¹ç”¨å–å†³äºä½ çš„ä»£ç†æœåŠ¡æä¾›å•†
///
/// ## æµ‹è¯•åœºæ™¯
/// 1. è·å–LLMå®¢æˆ·ç«¯å•ä¾‹
/// 2. æ„é€ è¯·æ±‚å‚æ•°ï¼ˆsystem prompt, user prompt, tokensç­‰ï¼‰
/// 3. é€šè¿‡é…ç½®çš„ä»£ç†URLè°ƒç”¨LLM APIï¼ˆè‡ªå®šä¹‰æ¨¡å‹ï¼‰
/// 4. ç­‰å¾…ä»£ç†æœåŠ¡å™¨å“åº”
/// 5. éªŒè¯å“åº”æ ¼å¼å’Œå†…å®¹
///
/// ## é¢„æœŸè¡Œä¸º
/// - æˆåŠŸæƒ…å†µï¼šè¿”å›`Ok(LLMResponse)`åŒ…å«AIç”Ÿæˆçš„å›å¤
/// - å¤±è´¥æƒ…å†µï¼šè¿”å›`Err(...)`å¹¶åŒ…å«æ¸…æ™°çš„é”™è¯¯ä¿¡æ¯ï¼š
///   - API keyæˆ–proxy URLæ— æ•ˆæˆ–ç¼ºå¤±
///   - ç½‘ç»œè¿æ¥é”™è¯¯
///   - ä»£ç†æœåŠ¡å™¨é”™è¯¯ï¼ˆ5xxï¼‰
///   - æ¨¡å‹ä¸å­˜åœ¨æˆ–æ— æƒé™
/// - å“åº”å†…å®¹ç¬¦åˆè¯·æ±‚çš„max_tokensé™åˆ¶
/// - æ­£ç¡®å¤„ç†ä»£ç†æœåŠ¡å™¨ç‰¹å®šçš„APIæ ¼å¼
/// - æ”¯æŒè‡ªå®šä¹‰æ¨¡å‹åç§°
#[test]
#[ignore] // éœ€è¦ç½‘ç»œè¯·æ±‚ï¼Œé»˜è®¤å¿½ç•¥
fn test_llm_client_call_with_proxy_provider() {
    // æµ‹è¯• call() æ–¹æ³• - Proxy providerï¼ˆè¦†ç›– client.rs:150-156, build_model:192ï¼‰
    use workflow::base::llm::types::LLMRequestParams;

    let client = LLMClient::global();
    let params = LLMRequestParams {
        system_prompt: "You are a helpful assistant.".to_string(),
        user_prompt: "Say hello".to_string(),
        max_tokens: Some(10),
        temperature: 0.5,
        model: "custom-model".to_string(),
    };

    // è¿™ä¸ªæµ‹è¯•éœ€è¦å®é™…çš„ proxy URL å’Œ API keyï¼Œæ‰€ä»¥é»˜è®¤å¿½ç•¥
    let result = client.call(&params);
    assert!(result.is_ok() || result.is_err()); // å¯èƒ½æˆåŠŸæˆ–å¤±è´¥ï¼Œå–å†³äºé…ç½®
}

#[test]
fn test_llm_client_build_payload_structure() {
    // æµ‹è¯• build_payload() æ–¹æ³•çš„ç»“æ„ï¼ˆé€šè¿‡ call æ–¹æ³•çš„é”™è¯¯æ¥é—´æ¥æµ‹è¯•ï¼‰
    // æ³¨æ„ï¼šè¿™ä¸ªæµ‹è¯•ä¼šå¤±è´¥ï¼Œå› ä¸ºéœ€è¦æœ‰æ•ˆçš„é…ç½®ï¼Œä½†å¯ä»¥éªŒè¯ payload æ„å»ºé€»è¾‘
    use workflow::base::llm::types::LLMRequestParams;

    let client = LLMClient::global();
    let params = LLMRequestParams {
        system_prompt: "System prompt".to_string(),
        user_prompt: "User prompt".to_string(),
        max_tokens: Some(100),
        temperature: 0.7,
        model: "test-model".to_string(),
    };

    // å°è¯•è°ƒç”¨ï¼Œå³ä½¿å¤±è´¥ä¹Ÿèƒ½éªŒè¯ build_payload çš„é€»è¾‘
    let result = client.call(&params);
    // å¦‚æœé…ç½®æ— æ•ˆï¼Œä¼šè¿”å›é”™è¯¯ï¼Œä½† build_payload çš„é€»è¾‘å·²ç»è¢«æ‰§è¡Œ
    assert!(result.is_ok() || result.is_err());
}

#[test]
fn test_llm_client_build_headers_structure() {
    // æµ‹è¯• build_headers() æ–¹æ³•çš„ç»“æ„ï¼ˆé€šè¿‡ call æ–¹æ³•çš„é”™è¯¯æ¥é—´æ¥æµ‹è¯•ï¼‰
    // æ³¨æ„ï¼šè¿™ä¸ªæµ‹è¯•ä¼šå¤±è´¥ï¼Œå› ä¸ºéœ€è¦æœ‰æ•ˆçš„é…ç½®ï¼Œä½†å¯ä»¥éªŒè¯ headers æ„å»ºé€»è¾‘
    use workflow::base::llm::types::LLMRequestParams;

    let client = LLMClient::global();
    let params = LLMRequestParams {
        system_prompt: "System prompt".to_string(),
        user_prompt: "User prompt".to_string(),
        max_tokens: None,
        temperature: 0.5,
        model: "test-model".to_string(),
    };

    // å°è¯•è°ƒç”¨ï¼Œå³ä½¿å¤±è´¥ä¹Ÿèƒ½éªŒè¯ build_headers çš„é€»è¾‘
    let result = client.call(&params);
    // å¦‚æœé…ç½®æ— æ•ˆï¼Œä¼šè¿”å›é”™è¯¯ï¼Œä½† build_headers çš„é€»è¾‘å·²ç»è¢«æ‰§è¡Œ
    assert!(result.is_ok() || result.is_err());
}
