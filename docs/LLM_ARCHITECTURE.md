# LLM ç»Ÿä¸€é…ç½®é©±åŠ¨æ¶æ„

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£æè¿°äº† Workflow CLI ä¸­ LLMï¼ˆå¤§è¯­è¨€æ¨¡å‹ï¼‰å®¢æˆ·ç«¯çš„ç»Ÿä¸€é…ç½®é©±åŠ¨æ¶æ„ã€‚è¯¥æ¶æ„é€šè¿‡**ç»Ÿä¸€å®¢æˆ·ç«¯**å’Œ**Settings é…ç½®ç³»ç»Ÿ**ï¼Œå®ç°æ‰€æœ‰ LLM æä¾›å•†çš„ç»Ÿä¸€è°ƒç”¨ï¼Œæ¶ˆé™¤ä»£ç é‡å¤ï¼Œæ”¯æŒé€šè¿‡ `workflow.toml` é…ç½®æ–‡ä»¶æŒä¹…åŒ–æä¾›å•†é…ç½®ã€‚

### æ ¸å¿ƒè®¾è®¡åŸåˆ™

1. **ç»Ÿä¸€å®¢æˆ·ç«¯**ï¼šæ‰€æœ‰ LLM æä¾›å•†ä½¿ç”¨åŒä¸€ä¸ªå®¢æˆ·ç«¯å®ç°
2. **é…ç½®é©±åŠ¨**ï¼šæ‰€æœ‰å‚æ•°ï¼ˆURLã€API Keyã€Modelã€Response Formatï¼‰ä» `Settings` åŠ¨æ€è·å–
3. **æ˜“äºæ‰©å±•**ï¼šæ·»åŠ æ–°çš„ LLM æä¾›å•†åªéœ€é…ç½®ï¼Œæ— éœ€å†™ä»£ç 
4. **å‘åå…¼å®¹**ï¼šä¿æŒç°æœ‰ API ä¸å˜ï¼Œæ”¯æŒä» `workflow.toml` é…ç½®

### ä¸ºä»€ä¹ˆé€‰æ‹©ç»Ÿä¸€é…ç½®é©±åŠ¨æ–¹æ¡ˆï¼Ÿ

åŸºäº API è°ƒç”¨åˆ†æï¼Œæ‰€æœ‰ LLM æä¾›å•†éƒ½éµå¾ª **OpenAI å…¼å®¹æ ¼å¼**ï¼š

- âœ… **è¯·æ±‚æ ¼å¼å®Œå…¨ç›¸åŒ**ï¼šéƒ½ä½¿ç”¨ POST åˆ° `/v1/chat/completions`ï¼Œè¯·æ±‚ä½“ç»“æ„ç›¸åŒ
- âœ… **å“åº”æ ¼å¼å®Œå…¨ç›¸åŒ**ï¼šéƒ½ä» `choices[0].message.content` æå–å†…å®¹
- âœ… **å”¯ä¸€å·®å¼‚**ï¼šURL å’Œ API Keyï¼ˆé…ç½®å·®å¼‚ï¼Œéä»£ç å·®å¼‚ï¼‰

**ç»“è®º**ï¼š**ä¸éœ€è¦ä¼ ç»Ÿæ’ä»¶ç³»ç»Ÿ**ï¼ˆtraitã€registryã€managerï¼‰ï¼Œåªéœ€è¦**é…ç½®é©±åŠ¨ + ç»Ÿä¸€å®¢æˆ·ç«¯**æ–¹æ¡ˆã€‚

---

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### æ•´ä½“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  PullRequestLLM                         â”‚
â”‚  (ä¸šåŠ¡å±‚ï¼šç”Ÿæˆ PR å†…å®¹)                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              LLMClient                                  â”‚
â”‚  (ç»Ÿä¸€å®¢æˆ·ç«¯ï¼Œå¤„ç†æ‰€æœ‰ LLM è°ƒç”¨)                          â”‚
â”‚  - build_url()     ä» Settings è·å– URL                  â”‚
â”‚  - build_model()   ä» Settings è·å– Model                â”‚
â”‚  - build_headers() ä» Settings è·å– API Key              â”‚
â”‚  - build_payload() æ„å»ºè¯·æ±‚ä½“                             â”‚
â”‚  - extract_content() æ ¹æ® response_format æå–å†…å®¹       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Settings                                   â”‚
â”‚  (é…ç½®ç³»ç»Ÿï¼šä» workflow.toml è¯»å–)                         â”‚
â”‚  - llm.provider     æä¾›å•†åç§° (openai/deepseek/proxy)   â”‚
â”‚  - llm.url          API URL (ä»… proxy éœ€è¦)               â”‚
â”‚  - llm.key          API Key                               â”‚
â”‚  - llm.model        æ¨¡å‹åç§°                               â”‚
â”‚  - llm.response_format å“åº”æ ¼å¼è·¯å¾„                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  HTTP Client   â”‚
            â”‚  (reqwest)     â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒç»„ä»¶

#### 1. LLMClientï¼ˆç»Ÿä¸€å®¢æˆ·ç«¯ï¼‰

æ‰€æœ‰ LLM æä¾›å•†ä½¿ç”¨åŒä¸€ä¸ªå®¢æˆ·ç«¯å®ç°ï¼Œé€šè¿‡ `Settings` é…ç½®åŒºåˆ†ä¸åŒçš„æä¾›å•†ï¼š

```rust
pub struct LLMClient;

impl LLMClient {
    pub fn new() -> Self {
        Self
    }

    pub fn call(&self, params: &LLMRequestParams) -> Result<String> {
        // 1. ä» Settings è·å–é…ç½®
        // 2. æ„å»º URLã€Headersã€Payload
        // 3. å‘é€ HTTP è¯·æ±‚
        // 4. æ ¹æ® response_format æå–å†…å®¹
    }
}
```

**å…³é”®ç‰¹æ€§**ï¼š
- âœ… **æ— çŠ¶æ€**ï¼šä¸å­˜å‚¨é…ç½®ï¼Œæ¯æ¬¡è°ƒç”¨æ—¶ä» `Settings::get()` è·å–
- âœ… **åŠ¨æ€é…ç½®**ï¼šæ‰€æœ‰é…ç½®ï¼ˆURLã€Keyã€Modelï¼‰éƒ½ä» `Settings` åŠ¨æ€è·å–
- âœ… **ç»Ÿä¸€å¤„ç†**ï¼šæ‰€æœ‰æä¾›å•†ä½¿ç”¨ç›¸åŒçš„è¯·æ±‚å’Œå“åº”å¤„ç†é€»è¾‘
- âœ… **è¶…æ—¶æ§åˆ¶**ï¼š60 ç§’è¶…æ—¶è®¾ç½®

#### 2. Settingsï¼ˆé…ç½®ç³»ç»Ÿï¼‰

é…ç½®å­˜å‚¨åœ¨ `workflow.toml` æ–‡ä»¶çš„ `[llm]` éƒ¨åˆ†ï¼š

```toml
[llm]
provider = "openai"  # æˆ– "deepseek" æˆ– "proxy"
key = "sk-xxx"        # API Key
model = "gpt-4.0"     # å¯é€‰ï¼Œopenai/deepseek æœ‰é»˜è®¤å€¼
url = "https://..."   # ä»… proxy éœ€è¦
response_format = "choices[0].message.content"  # å¯é€‰ï¼Œæœ‰é»˜è®¤å€¼
```

**é…ç½®å­—æ®µè¯´æ˜**ï¼š

| å­—æ®µ | ç±»å‹ | å¿…éœ€ | è¯´æ˜ |
|------|------|------|------|
| `provider` | String | âœ… | æä¾›å•†åç§°ï¼š`openai`ã€`deepseek`ã€`proxy` |
| `key` | String | âœ… | API Key |
| `url` | String | âš ï¸ | API URLï¼ˆä»… `proxy` æä¾›å•†éœ€è¦ï¼‰ |
| `model` | String | âš ï¸ | æ¨¡å‹åç§°ï¼ˆ`openai`/`deepseek` æœ‰é»˜è®¤å€¼ï¼Œ`proxy` å¿…å¡«ï¼‰ |
| `response_format` | String | âŒ | å“åº”æ ¼å¼è·¯å¾„ï¼ˆé»˜è®¤ï¼š`choices[0].message.content`ï¼‰ |

**é»˜è®¤å€¼**ï¼š

- `provider`: `"openai"`
- `model`:
  - `openai`: `"gpt-4.0"`
  - `deepseek`: `"deepseek-chat"`
  - `proxy`: æ— é»˜è®¤å€¼ï¼Œå¿…é¡»é…ç½®
- `response_format`: `"choices[0].message.content"`

#### 3. PullRequestLLMï¼ˆä¸šåŠ¡å±‚ï¼‰

ä½¿ç”¨ç»Ÿä¸€å®¢æˆ·ç«¯ç”Ÿæˆ PR å†…å®¹ï¼š

```rust
impl PullRequestLLM {
    pub fn generate(...) -> Result<PullRequestContent> {
        let client = LLMClient::new();
        let params = LLMRequestParams { ... };
        let response = client.call(&params)?;
        Self::parse_llm_response(response)
    }
}
```

---

## ğŸ”Œ é…ç½®æ–¹å¼

### é…ç½®æ–‡ä»¶ï¼ˆæ¨èï¼‰

é€šè¿‡ `workflow setup` å‘½ä»¤äº¤äº’å¼é…ç½®ï¼Œæˆ–ç›´æ¥ç¼–è¾‘ `workflow.toml`ï¼š

```toml
[llm]
provider = "openai"
key = "sk-xxx"
model = "gpt-4.0"
```

### æ”¯æŒçš„æä¾›å•†

#### OpenAI

```toml
[llm]
provider = "openai"
key = "sk-xxx"
model = "gpt-4.0"  # å¯é€‰ï¼Œé»˜è®¤ "gpt-4.0"
```

**è‡ªåŠ¨é…ç½®**ï¼š
- URL: `https://api.openai.com/v1/chat/completions`ï¼ˆè‡ªåŠ¨è®¾ç½®ï¼Œæ— éœ€é…ç½®ï¼‰

#### DeepSeek

```toml
[llm]
provider = "deepseek"
key = "sk-xxx"
model = "deepseek-chat"  # å¯é€‰ï¼Œé»˜è®¤ "deepseek-chat"
```

**è‡ªåŠ¨é…ç½®**ï¼š
- URL: `https://api.deepseek.com/chat/completions`ï¼ˆè‡ªåŠ¨è®¾ç½®ï¼Œæ— éœ€é…ç½®ï¼‰

#### Proxyï¼ˆä»£ç† APIï¼‰

```toml
[llm]
provider = "proxy"
url = "https://proxy.example.com"  # å¿…éœ€
key = "your-api-key"                # å¿…éœ€
model = "qwen-3-235b"               # å¿…éœ€
```

**è‡ªåŠ¨é…ç½®**ï¼š
- URL: `{url}/chat/completions`ï¼ˆè‡ªåŠ¨æ‹¼æ¥ `/chat/completions`ï¼‰

---

## ğŸ“ æ–‡ä»¶ç»“æ„

```
src/lib/llm/
â”œâ”€â”€ mod.rs                    # æ¨¡å—å¯¼å‡º
â”œâ”€â”€ pr_llm.rs                 # PullRequestLLMï¼ˆä¸šåŠ¡å±‚ï¼‰
â””â”€â”€ client/
    â”œâ”€â”€ mod.rs                 # å®¢æˆ·ç«¯æ¨¡å—å¯¼å‡º
    â”œâ”€â”€ llm_client.rs         # LLMClientï¼ˆç»Ÿä¸€å®¢æˆ·ç«¯ï¼‰
    â””â”€â”€ types.rs              # å…±äº«ç±»å‹ï¼ˆLLMRequestParamsï¼‰
```

---

## ğŸ”§ å®ç°ç»†èŠ‚

### 1. URL æ„å»º

æ ¹æ® `provider` åŠ¨æ€æ„å»º URLï¼š

```rust
fn build_url(&self) -> Result<String> {
    let settings = Settings::get();
    match settings.llm.provider.as_str() {
        "openai" => Ok("https://api.openai.com/v1/chat/completions".to_string()),
        "deepseek" => Ok("https://api.deepseek.com/chat/completions".to_string()),
        "proxy" => {
            let base_url = settings.llm.url.as_ref()
                .context("LLM proxy URL is not configured")?;
            Ok(format!("{}/chat/completions", base_url.trim_end_matches('/')))
        }
        _ => Err(anyhow::anyhow!("Unsupported LLM provider: {}", provider)),
    }
}
```

### 2. Model æ„å»º

æ ¹æ® `provider` è·å–æ¨¡å‹åç§°ï¼š

```rust
fn build_model(&self) -> Result<String> {
    let settings = Settings::get();
    match settings.llm.provider.as_str() {
        "openai" | "deepseek" => {
            Ok(settings.llm.model.clone()
                .unwrap_or_else(|| default_llm_model(&settings.llm.provider)))
        }
        "proxy" => {
            settings.llm.model.clone()
                .context("Model is required for proxy provider")
        }
        _ => Err(anyhow::anyhow!("Unsupported LLM provider")),
    }
}
```

### 3. å“åº”å†…å®¹æå–

æ ¹æ® `response_format` é…ç½®æå–å†…å®¹ï¼š

```rust
fn extract_content(&self, response: &serde_json::Value) -> Result<String> {
    let settings = Settings::get();
    let response_format = &settings.llm.response_format;

    if response_format.is_empty() || *response_format == default_response_format() {
        // æ ‡å‡† OpenAI æ ¼å¼ï¼šchoices[0].message.content
        response
            .get("choices")
            .and_then(|c| c.as_array())
            .and_then(|arr| arr.first())
            .and_then(|choice| choice.get("message"))
            .and_then(|msg| msg.get("content"))
            .and_then(|c| c.as_str())
            .context("Failed to extract content")
            .map(|s| s.trim().to_string())
    } else {
        // è‡ªå®šä¹‰ JSON path æå–
        self.extract_by_path(response, response_format)
    }
}
```

**æ”¯æŒçš„å“åº”æ ¼å¼**ï¼š
- **æ ‡å‡†æ ¼å¼**ï¼š`choices[0].message.content`ï¼ˆé»˜è®¤ï¼‰
- **è‡ªå®šä¹‰æ ¼å¼**ï¼šæ”¯æŒ JSON pathï¼Œå¦‚ `candidates[0].content.parts[0].text`

### 4. é”™è¯¯å¤„ç†

- **API Key ä¸ºç©º**ï¼šè¿”å›é”™è¯¯ `"LLM key is empty in settings"`
- **Proxy URL æœªé…ç½®**ï¼šè¿”å›é”™è¯¯ `"LLM proxy URL is not configured"`
- **Proxy Model æœªé…ç½®**ï¼šè¿”å›é”™è¯¯ `"Model is required for proxy provider"`
- **HTTP è¯·æ±‚å¤±è´¥**ï¼šè¿”å›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ï¼ŒåŒ…å«çŠ¶æ€ç å’Œå“åº”ä½“

---

## âœ… ä¼˜åŠ¿åˆ†æ

### 1. ç»Ÿä¸€å®¢æˆ·ç«¯
- âœ… æ‰€æœ‰ LLM æä¾›å•†ä½¿ç”¨åŒä¸€ä¸ªå®¢æˆ·ç«¯å®ç°
- âœ… **æ¶ˆé™¤ä»£ç é‡å¤**ï¼šä» ~300 è¡Œï¼ˆ3 ä¸ªç‹¬ç«‹å®¢æˆ·ç«¯ï¼‰å‡å°‘åˆ° ~450 è¡Œï¼ˆ1 ä¸ªç»Ÿä¸€å®¢æˆ·ç«¯ï¼‰
- âœ… ç»Ÿä¸€çš„é”™è¯¯å¤„ç†å’Œè¯·æ±‚é€»è¾‘

### 2. é…ç½®é©±åŠ¨
- âœ… æ‰€æœ‰å‚æ•°ä» `Settings` è¯»å–ï¼ˆURLã€API Keyã€Modelã€Response Formatï¼‰
- âœ… **æ˜“äºæ‰©å±•**ï¼šæ·»åŠ æ–°æä¾›å•†åªéœ€é…ç½®ï¼Œæ— éœ€å†™ä»£ç 
- âœ… æ”¯æŒè‡ªå®šä¹‰å“åº”æ ¼å¼ï¼ˆé€šè¿‡ JSON pathï¼‰

### 3. æŒä¹…åŒ–é…ç½®
- âœ… é…ç½®å­˜å‚¨åœ¨ `workflow.toml`ï¼Œä¸é¡¹ç›®é…ç½®ç»Ÿä¸€ç®¡ç†
- âœ… é€šè¿‡ `workflow setup` äº¤äº’å¼é…ç½®
- âœ… æ”¯æŒç‰ˆæœ¬æ§åˆ¶ï¼ˆå¯ä»¥æäº¤åˆ° Git ä»“åº“ï¼‰

### 4. å‘åå…¼å®¹
- âœ… ä¿æŒç°æœ‰ API ä¸å˜
- âœ… å¹³æ»‘è¿ç§»è·¯å¾„
- âœ… é…ç½®ç®€å•ç›´è§‚

### 5. ç»´æŠ¤æˆæœ¬ä½
- âœ… **å•ä¸€ä»£ç è·¯å¾„**ï¼šæ‰€æœ‰å®¢æˆ·ç«¯éƒ½èµ°ç»Ÿä¸€å®¢æˆ·ç«¯
- âœ… **æ— éœ€ç»´æŠ¤å¤šå¥—å®ç°**ï¼šæ·»åŠ å¤šä¸ªå®¢æˆ·ç«¯æ—¶ï¼Œåªéœ€é…ç½®
- âœ… **ä»£ç ä¸€è‡´æ€§é«˜**ï¼šæ‰€æœ‰å®¢æˆ·ç«¯ä½¿ç”¨ç›¸åŒçš„é€»è¾‘

---

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹ 1ï¼šåŸºæœ¬ä½¿ç”¨

```rust
use crate::llm::pr_llm::PullRequestLLM;

let content = PullRequestLLM::generate(
    "Fix login bug",
    None,
    None
)?;

println!("Branch: {}", content.branch_name);
println!("PR Title: {}", content.pr_title);
```

### ç¤ºä¾‹ 2ï¼šé…ç½® OpenAI

```toml
# workflow.toml
[llm]
provider = "openai"
key = "sk-xxx"
model = "gpt-4.0"
```

### ç¤ºä¾‹ 3ï¼šé…ç½® Proxy

```toml
# workflow.toml
[llm]
provider = "proxy"
url = "https://proxy.example.com"
key = "your-api-key"
model = "qwen-3-235b"
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [CONFIG_ARCHITECTURE.md](./CONFIG_ARCHITECTURE.md) - Settings é…ç½®ç³»ç»Ÿæ¶æ„
- [ARCHITECTURE.md](./ARCHITECTURE.md) - æ€»ä½“æ¶æ„è®¾è®¡æ–‡æ¡£

---

## ğŸ” æ€»ç»“

è¯¥ç»Ÿä¸€é…ç½®é©±åŠ¨æ–¹æ¡ˆæä¾›äº†ï¼š

1. **ç»Ÿä¸€å®¢æˆ·ç«¯**ï¼šæ‰€æœ‰ LLM æä¾›å•†ä½¿ç”¨åŒä¸€ä¸ªå®¢æˆ·ç«¯å®ç°ï¼Œæ¶ˆé™¤ä»£ç é‡å¤
2. **é…ç½®é©±åŠ¨**ï¼šæ‰€æœ‰å‚æ•°ä» `Settings` è¯»å–ï¼Œæ·»åŠ æ–°æä¾›å•†åªéœ€é…ç½®
3. **æŒä¹…åŒ–é…ç½®**ï¼šé€šè¿‡ `workflow.toml` æ”¯æŒæä¾›å•†é…ç½®çš„æŒä¹…åŒ–å­˜å‚¨
4. **æ˜“äºæ‰©å±•**ï¼šæ·»åŠ æ–°æä¾›å•†åªéœ€é…ç½®ï¼Œæ— éœ€å†™ä»£ç 
5. **å‘åå…¼å®¹**ï¼šä¿æŒç°æœ‰ API ä¸å˜
6. **ç»´æŠ¤æˆæœ¬ä½**ï¼šåªéœ€ç»´æŠ¤ä¸€ä¸ªç»Ÿä¸€å®¢æˆ·ç«¯

### æ ¸å¿ƒåŸåˆ™

- **é…ç½®é©±åŠ¨**ï¼šæ‰€æœ‰å·®å¼‚é€šè¿‡é…ç½®è§£å†³
- **ç»Ÿä¸€å®ç°**ï¼šæ‰€æœ‰æä¾›å•†ä½¿ç”¨åŒä¸€ä¸ªå®¢æˆ·ç«¯
- **ç®€å•é«˜æ•ˆ**ï¼šæ— éœ€å¤æ‚çš„æ’ä»¶ç³»ç»Ÿ

### å½“å‰å®ç°çŠ¶æ€

âœ… **å·²å®ç°**ï¼š
- ç»Ÿä¸€ `LLMClient` å®ç°
- åŸºäº `Settings` çš„é…ç½®ç³»ç»Ÿ
- æ”¯æŒ OpenAIã€DeepSeekã€Proxy æä¾›å•†
- è‡ªå®šä¹‰å“åº”æ ¼å¼æ”¯æŒï¼ˆJSON pathï¼‰

âš ï¸ **é…ç½®è¯´æ˜**ï¼š
- å½“å‰å®ç°ä½¿ç”¨ `workflow.toml` çš„ `[llm]` éƒ¨åˆ†è¿›è¡Œé…ç½®
- æ‰€æœ‰ LLM ç›¸å…³é…ç½®ç»Ÿä¸€å­˜å‚¨åœ¨ `workflow.toml` ä¸­ï¼Œä¸é¡¹ç›®é…ç½®ç»Ÿä¸€ç®¡ç†

