# LLM ç»Ÿä¸€é…ç½®é©±åŠ¨å®ç°æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›äº† LLM ç»Ÿä¸€é…ç½®é©±åŠ¨æ–¹æ¡ˆçš„è¯¦ç»†å®ç°æŒ‡å—ï¼ŒåŒ…å«å®Œæ•´çš„ä»£ç ç»“æ„ã€API å®šä¹‰ã€å®ç°æ­¥éª¤å’Œæµ‹è¯•ç­–ç•¥ã€‚

**å‚è€ƒæ–‡æ¡£**ï¼š
- [LLM_PLUGIN_ARCHITECTURE.md](./LLM_PLUGIN_ARCHITECTURE.md) - æ¶æ„è®¾è®¡æ–‡æ¡£
- [LLM_PLUGIN_CURL.md](./LLM_PLUGIN_CURL.md) - API è°ƒç”¨ç¤ºä¾‹

---

## ğŸ“¦ ä¾èµ–é¡¹

### éœ€è¦æ·»åŠ çš„ä¾èµ–

åœ¨ `Cargo.toml` ä¸­æ·»åŠ ï¼š

```toml
[dependencies]
# ... ç°æœ‰ä¾èµ– ...
toml = "0.8"  # TOML é…ç½®æ–‡ä»¶è§£æ
dirs = "5.0"  # ç”¨äºæŸ¥æ‰¾é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œå¦‚æœå·²æœ‰åˆ™ä¸éœ€è¦ï¼‰
```

**æ³¨æ„**ï¼šæ£€æŸ¥é¡¹ç›®ä¸­æ˜¯å¦å·²æœ‰ `dirs` ä¾èµ–ï¼Œå¦‚æœæ²¡æœ‰åˆ™æ·»åŠ ã€‚

---

## ğŸ“ æ–‡ä»¶ç»“æ„

```
src/lib/llm/
â”œâ”€â”€ mod.rs                    # æ¨¡å—å¯¼å‡º
â”œâ”€â”€ pr_llm.rs                 # PullRequestLLMï¼ˆä¸šåŠ¡å±‚ï¼Œéœ€è¦æ›´æ–°ï¼‰
â””â”€â”€ client/
    â”œâ”€â”€ mod.rs                 # å®¢æˆ·ç«¯æ¨¡å—å¯¼å‡º
    â”œâ”€â”€ client.rs              # LLMClientï¼ˆç»Ÿä¸€å®¢æˆ·ç«¯ï¼‰â­ æ–°å»º
    â”œâ”€â”€ config.rs              # é…ç½®æ–‡ä»¶åŠ è½½å’Œè§£æ â­ æ–°å»º
    â””â”€â”€ common.rs              # å…±äº«ç±»å‹å’Œå·¥å…· â­ æ–°å»º
```

**è¿ç§»è®¡åˆ’**ï¼š
- ä¿ç•™ `openai.rs`ã€`deepseek.rs`ã€`proxy.rs` ä½œä¸ºå‘åå…¼å®¹ï¼ˆæ ‡è®°ä¸º deprecatedï¼‰
- æ–°å»º `client.rs`ã€`config.rs`ã€`common.rs`
- æ›´æ–° `pr_llm.rs` ä½¿ç”¨ç»Ÿä¸€å®¢æˆ·ç«¯

**å‘½åè¯´æ˜**ï¼š
- `client.rs`ï¼šç»Ÿä¸€å®¢æˆ·ç«¯å®ç°ï¼ˆå‚è€ƒé¡¹ç›®ä¸­çš„ `src/lib/http/client.rs` å‘½åä¹ æƒ¯ï¼‰
- `config.rs`ï¼šé…ç½®æ–‡ä»¶åŠ è½½å’Œè§£æ
- `common.rs`ï¼šå…±äº«ç±»å‹å’Œå·¥å…·

---

## ğŸ”§ å®ç°æ­¥éª¤

### æ­¥éª¤ 1ï¼šåˆ›å»ºå…±äº«ç±»å‹ï¼ˆcommon.rsï¼‰

**æ–‡ä»¶**ï¼š`src/lib/llm/client/common.rs`

```rust
//! LLM å®¢æˆ·ç«¯å…±äº«ç±»å‹å’Œå·¥å…·

use serde::{Deserialize, Serialize};

/// LLM è¯·æ±‚å‚æ•°
///
/// åŒ…å«è°ƒç”¨ LLM API æ‰€éœ€çš„æ‰€æœ‰å‚æ•°ã€‚
#[derive(Debug, Clone, Serialize)]
pub struct LLMRequestParams {
    /// ç³»ç»Ÿæç¤ºè¯
    pub system_prompt: String,
    /// ç”¨æˆ·æç¤ºè¯
    pub user_prompt: String,
    /// æœ€å¤§ token æ•°
    pub max_tokens: u32,
    /// æ¸©åº¦å‚æ•°ï¼ˆæ§åˆ¶è¾“å‡ºçš„éšæœºæ€§ï¼‰
    pub temperature: f32,
    /// æ¨¡å‹åç§°ï¼ˆå¦‚ "gpt-3.5-turbo"ï¼‰
    pub model: String,
}

impl Default for LLMRequestParams {
    fn default() -> Self {
        Self {
            system_prompt: String::new(),
            user_prompt: String::new(),
            max_tokens: 100,
            temperature: 0.5,
            model: "gpt-3.5-turbo".to_string(),
        }
    }
}
```

---

### æ­¥éª¤ 2ï¼šå®ç°ç»Ÿä¸€å®¢æˆ·ç«¯ï¼ˆclient.rsï¼‰

**æ–‡ä»¶**ï¼š`src/lib/llm/client/client.rs`

```rust
//! LLM å®¢æˆ·ç«¯
//!
//! æœ¬æ¨¡å—æä¾›äº† LLM å®¢æˆ·ç«¯å®ç°ï¼Œæ”¯æŒæ‰€æœ‰éµå¾ª OpenAI å…¼å®¹æ ¼å¼çš„æä¾›å•†ã€‚

use anyhow::{Context, Result};
use reqwest::header::{HeaderMap, HeaderValue, AUTHORIZATION, CONTENT_TYPE};
use serde_json::json;

use crate::http::{HttpClient, HttpResponse};
use super::common::LLMRequestParams;

/// å“åº”æ ¼å¼
#[derive(Debug, Clone)]
pub enum ResponseFormat {
    /// OpenAI æ ‡å‡†æ ¼å¼ï¼šchoices[0].message.content
    OpenAI,
    /// è‡ªå®šä¹‰æ ¼å¼ï¼šé€šè¿‡ JSON path æå–
    Custom {
        content_path: String,
        error_path: Option<String>,
    },
}

/// LLM å®¢æˆ·ç«¯é…ç½®
#[derive(Debug, Clone)]
pub struct LLMClientConfig {
    pub url: String,
    pub api_key: String,
    pub response_format: ResponseFormat,
    pub timeout: Option<u64>,
    pub retry_count: Option<u32>,
}

/// LLM å®¢æˆ·ç«¯
///
/// æ‰€æœ‰ LLM æä¾›å•†ä½¿ç”¨åŒä¸€ä¸ªå®¢æˆ·ç«¯å®ç°ï¼Œé€šè¿‡é…ç½®åŒºåˆ†ä¸åŒçš„æä¾›å•†ã€‚
pub struct LLMClient {
    name: String,
    config: LLMClientConfig,
}

impl LLMClient {
    /// ä»é…ç½®åˆ›å»ºå®¢æˆ·ç«¯
    pub fn from_config(name: String, config: LLMClientConfig) -> Self {
        Self { name, config }
    }

    /// è°ƒç”¨ LLM API
    ///
    /// # å‚æ•°
    ///
    /// * `params` - LLM è¯·æ±‚å‚æ•°
    ///
    /// # è¿”å›
    ///
    /// è¿”å› LLM ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹ï¼ˆå»é™¤é¦–å°¾ç©ºç™½ï¼‰ã€‚
    ///
    /// # é”™è¯¯
    ///
    /// å¦‚æœ API è°ƒç”¨å¤±è´¥æˆ–å“åº”æ ¼å¼ä¸æ­£ç¡®ï¼Œè¿”å›ç›¸åº”çš„é”™è¯¯ä¿¡æ¯ã€‚
    pub fn call(&self, params: &LLMRequestParams) -> Result<String> {
        let client = HttpClient::new()?;

        // æ„å»ºè¯·æ±‚ä½“ï¼ˆç»Ÿä¸€æ ¼å¼ï¼‰
        let payload = self.build_payload(params);

        // æ„å»ºè¯·æ±‚å¤´ï¼ˆç»Ÿä¸€æ ¼å¼ï¼‰
        let headers = self.build_headers()?;

        // å‘é€è¯·æ±‚
        let response: HttpResponse<serde_json::Value> = client
            .post(&self.config.url, &payload, None, Some(&headers))
            .with_context(|| format!("Failed to send LLM request to {}", self.name))?;

        // æ£€æŸ¥é”™è¯¯
        if !response.is_success() {
            return self.handle_error(&response);
        }

        // æ ¹æ®é…ç½®çš„å“åº”æ ¼å¼æå–å†…å®¹
        self.extract_content(&response.data)
    }

    /// æ„å»ºè¯·æ±‚ä½“
    fn build_payload(&self, params: &LLMRequestParams) -> serde_json::Value {
        json!({
            "model": params.model,
            "messages": [
                {
                    "role": "system",
                    "content": params.system_prompt
                },
                {
                    "role": "user",
                    "content": params.user_prompt
                }
            ],
            "max_tokens": params.max_tokens,
            "temperature": params.temperature
        })
    }

    /// æ„å»ºè¯·æ±‚å¤´
    fn build_headers(&self) -> Result<HeaderMap> {
        let mut headers = HeaderMap::new();
        headers.insert(
            AUTHORIZATION,
            HeaderValue::from_str(&format!("Bearer {}", self.config.api_key))
                .context("Failed to create Authorization header")?,
        );
        headers.insert(CONTENT_TYPE, HeaderValue::from_static("application/json"));
        Ok(headers)
    }

    /// ä»å“åº”ä¸­æå–å†…å®¹
    fn extract_content(&self, response: &serde_json::Value) -> Result<String> {
        match &self.config.response_format {
            ResponseFormat::OpenAI => {
                // æ ‡å‡† OpenAI æ ¼å¼
                response
                    .get("choices")
                    .and_then(|c| c.as_array())
                    .and_then(|arr| arr.first())
                    .and_then(|choice| choice.get("message"))
                    .and_then(|msg| msg.get("content"))
                    .and_then(|c| c.as_str())
                    .context("Failed to extract content from OpenAI format response")
                    .map(|s| s.trim().to_string())
            }
            ResponseFormat::Custom { content_path, .. } => {
                // é€šè¿‡ JSON path æå–
                self.extract_by_path(response, content_path)
            }
        }
    }

    /// é€šè¿‡ JSON path æå–å†…å®¹
    fn extract_by_path(&self, json: &serde_json::Value, path: &str) -> Result<String> {
        // ä¾‹å¦‚: "data.result.text" -> json["data"]["result"]["text"]
        let parts: Vec<&str> = path.split('.').collect();
        let mut current = json;

        for part in parts {
            current = current
                .get(part)
                .with_context(|| format!("Path '{}' not found in response", path))?;
        }

        current
            .as_str()
            .with_context(|| format!("Value at path '{}' is not a string", path))
            .map(|s| s.trim().to_string())
    }

    /// å¤„ç†é”™è¯¯å“åº”
    fn handle_error(&self, response: &HttpResponse<serde_json::Value>) -> Result<String> {
        let error_text = serde_json::to_string(&response.data).unwrap_or_default();
        anyhow::bail!(
            "LLM API request failed ({}): {} - {}",
            self.name,
            response.status,
            error_text
        );
    }
}
```

---

### æ­¥éª¤ 3ï¼šå®ç°é…ç½®åŠ è½½ï¼ˆconfig.rsï¼‰

**æ–‡ä»¶**ï¼š`src/lib/llm/client/config.rs`

```rust
//! LLM é…ç½®æ–‡ä»¶åŠ è½½å’Œè§£æ

use anyhow::{Context, Result};
use std::env;
use std::path::{Path, PathBuf};
use toml::Value;

use super::client::{LLMClientConfig, ResponseFormat, LLMClient};

/// æ’ä»¶é…ç½®æ¡ç›®
#[derive(Debug)]
pub struct PluginEntry {
    pub name: String,
    pub enabled: bool,
    pub config: Value,  // ä½¿ç”¨ Value æ”¯æŒçµæ´»é…ç½®
    pub advanced: Option<Value>,
}

/// LLM æ’ä»¶é…ç½®
///
/// ä» TOML é…ç½®æ–‡ä»¶åŠ è½½çš„ LLM æä¾›å•†é…ç½®ã€‚
#[derive(Debug)]
pub struct LLMConfig {
    pub version: String,
    pub default_plugin: Option<String>,
    pub plugins: Vec<PluginEntry>,
}

impl LLMConfig {
    /// æŸ¥æ‰¾é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
    pub fn find_config_path() -> Option<PathBuf> {
        // 1. æ£€æŸ¥ç¯å¢ƒå˜é‡ WORKFLOW_LLM_PLUGINS_CONFIG
        if let Ok(path) = env::var("WORKFLOW_LLM_PLUGINS_CONFIG") {
            let path = PathBuf::from(path);
            if path.exists() {
                return Some(path);
            }
        }

        // 2. æ£€æŸ¥å½“å‰ç›®å½•çš„ llm.toml
        if let Ok(current_dir) = env::current_dir() {
            let project_config = current_dir.join("llm.toml");
            if project_config.exists() {
                return Some(project_config);
            }
        }

        // 3. æ£€æŸ¥ XDG_CONFIG_HOME/workflow/llm.toml
        if let Ok(xdg_config) = env::var("XDG_CONFIG_HOME") {
            let xdg_path = PathBuf::from(xdg_config)
                .join("workflow")
                .join("llm.toml");
            if xdg_path.exists() {
                return Some(xdg_path);
            }
        }

        // 4. æ£€æŸ¥ ~/.workflow/llm.toml
        if let Some(home) = dirs::home_dir() {
            let home_config = home.join(".workflow").join("llm.toml");
            if home_config.exists() {
                return Some(home_config);
            }
        }

        None
    }

    /// ä»æ–‡ä»¶åŠ è½½é…ç½®
    pub fn load(path: &Path) -> Result<Self> {
        let content = std::fs::read_to_string(path)
            .with_context(|| format!("Failed to read config file: {}", path.display()))?;

        // è§£æç¯å¢ƒå˜é‡å¼•ç”¨
        let content = Self::resolve_env_vars(&content)?;

        // è§£æ TOML
        let value: Value = toml::from_str(&content)
            .with_context(|| format!("Failed to parse config file: {}", path.display()))?;

        // è½¬æ¢ä¸º LLMConfig
        let config = Self::from_toml_value(value)?;

        // éªŒè¯é…ç½®
        config.validate()?;

        Ok(config)
    }

    /// ä» TOML Value åˆ›å»º LLMConfig
    fn from_toml_value(value: Value) -> Result<Self> {
        let version = value
            .get("version")
            .and_then(|v| v.as_str())
            .context("Missing 'version' field")?
            .to_string();

        let default_plugin = value
            .get("default_plugin")
            .and_then(|v| v.as_str())
            .map(|s| s.to_string());

        let plugins_array = value
            .get("plugins")
            .and_then(|v| v.as_array())
            .context("Missing 'plugins' array")?;

        let mut plugins = Vec::new();
        for plugin_value in plugins_array {
            let name = plugin_value
                .get("name")
                .and_then(|v| v.as_str())
                .context("Missing 'name' field in plugin")?
                .to_string();

            let enabled = plugin_value
                .get("enabled")
                .and_then(|v| v.as_bool())
                .unwrap_or(true);

            let config = plugin_value
                .get("config")
                .context("Missing 'config' field in plugin")?
                .clone();

            let advanced = plugin_value.get("advanced").cloned();

            plugins.push(PluginEntry {
                name,
                enabled,
                config,
                advanced,
            });
        }

        Ok(LLMConfig {
            version,
            default_plugin,
            plugins,
        })
    }

    /// è§£æç¯å¢ƒå˜é‡å¼•ç”¨
    ///
    /// æ”¯æŒ `${VAR_NAME}` å’Œ `${VAR_NAME:default}` æ ¼å¼ã€‚
    fn resolve_env_vars(content: &str) -> Result<String> {
        use regex::Regex;

        // åŒ¹é… ${VAR_NAME} æˆ– ${VAR_NAME:default}
        let re = Regex::new(r"\$\{([^}:]+)(?::([^}]*))?\}")?;

        let result = re.replace_all(content, |caps: &regex::Captures| {
            let var_name = caps.get(1).unwrap().as_str();
            let default = caps.get(2).map(|m| m.as_str());

            match env::var(var_name) {
                Ok(value) => value,
                Err(_) => {
                    if let Some(default_val) = default {
                        default_val.to_string()
                    } else {
                        // å¦‚æœç¯å¢ƒå˜é‡ä¸å­˜åœ¨ä¸”æ²¡æœ‰é»˜è®¤å€¼ï¼Œä¿æŒåŸæ ·ï¼ˆåç»­éªŒè¯ä¼šæŠ¥é”™ï¼‰
                        caps.get(0).unwrap().as_str().to_string()
                    }
                }
            }
        });

        Ok(result.to_string())
    }

    /// éªŒè¯é…ç½®
    fn validate(&self) -> Result<()> {
        // éªŒè¯ç‰ˆæœ¬
        if self.version != "1.0" {
            return Err(anyhow::anyhow!("Unsupported config version: {}", self.version));
        }

        // éªŒè¯æ’ä»¶é…ç½®
        for plugin in &self.plugins {
            if !plugin.enabled {
                continue;
            }

            // éªŒè¯å¿…å¡«å­—æ®µï¼ˆæ‰€æœ‰æ’ä»¶éƒ½éœ€è¦ url å’Œ api_keyï¼‰
            Self::require_field(&plugin.config, "url")?;
            Self::require_field(&plugin.config, "api_key")?;
        }

        // éªŒè¯ default_plugin æ˜¯å¦å­˜åœ¨
        if let Some(ref default) = self.default_plugin {
            if !self.plugins.iter().any(|p| p.name == *default && p.enabled) {
                return Err(anyhow::anyhow!(
                    "Default plugin '{}' not found or disabled",
                    default
                ));
            }
        }

        Ok(())
    }

    fn require_field(config: &Value, field: &str) -> Result<()> {
        if !config.get(field).is_some() {
            return Err(anyhow::anyhow!("Missing required field: {}", field));
        }
        Ok(())
    }

    /// è·å–æŒ‡å®šåç§°çš„å®¢æˆ·ç«¯
    pub fn get_client(&self, name: &str) -> Result<LLMClient> {
        let plugin = self.plugins
            .iter()
            .find(|p| p.name == name && p.enabled)
            .with_context(|| format!("Plugin '{}' not found or disabled", name))?;

        let config = Self::parse_client_config(&plugin.config)?;
        Ok(LLMClient::from_config(plugin.name.clone(), config))
    }

    /// è·å–é»˜è®¤å®¢æˆ·ç«¯
    pub fn get_default_client(&self) -> Result<LLMClient> {
        if let Some(ref default_name) = self.default_plugin {
            return self.get_client(default_name);
        }

        // å¦‚æœæ²¡æœ‰æŒ‡å®šé»˜è®¤æ’ä»¶ï¼Œè¿”å›ç¬¬ä¸€ä¸ªå¯ç”¨çš„æ’ä»¶
        let plugin = self.plugins
            .iter()
            .find(|p| p.enabled)
            .context("No enabled plugins found")?;

        let config = Self::parse_client_config(&plugin.config)?;
        Ok(LLMClient::from_config(plugin.name.clone(), config))
    }

    /// ä»ç¯å¢ƒå˜é‡åˆ›å»ºé»˜è®¤é…ç½®ï¼ˆå‘åå…¼å®¹ï¼‰
    pub fn from_env() -> Result<Option<LLMClient>> {
        use crate::settings::Settings;

        let settings = Settings::load();
        let provider = settings.llm_provider.as_str();

        let config = match provider {
            "openai" => {
                let api_key = settings.openai_key
                    .context("LLM_OPENAI_KEY not set")?;
                LLMClientConfig {
                    url: "https://api.openai.com/v1/chat/completions".to_string(),
                    api_key,
                    response_format: ResponseFormat::OpenAI,
                    timeout: None,
                    retry_count: None,
                }
            }
            "deepseek" => {
                let api_key = settings.deepseek_key
                    .context("LLM_DEEPSEEK_KEY not set")?;
                LLMClientConfig {
                    url: "https://api.deepseek.com/v1/chat/completions".to_string(),
                    api_key,
                    response_format: ResponseFormat::OpenAI,
                    timeout: None,
                    retry_count: None,
                }
            }
            "proxy" => {
                let api_key = settings.llm_proxy_key
                    .context("LLM_PROXY_KEY not set")?;
                let base_url = settings.llm_proxy_url
                    .context("LLM_PROXY_URL not set")?;
                let url = format!("{}/chat/completions", base_url.trim_end_matches('/'));
                LLMClientConfig {
                    url,
                    api_key,
                    response_format: ResponseFormat::OpenAI,
                    timeout: None,
                    retry_count: None,
                }
            }
            _ => return Ok(None),
        };

        Ok(Some(LLMClient::from_config(
            provider.to_string(),
            config,
        )))
    }

    /// è§£æå®¢æˆ·ç«¯é…ç½®
    fn parse_client_config(config: &Value) -> Result<LLMClientConfig> {
        let url = config
            .get("url")
            .and_then(|v| v.as_str())
            .context("Missing 'url' field")?
            .to_string();

        let api_key = config
            .get("api_key")
            .and_then(|v| v.as_str())
            .context("Missing 'api_key' field")?
            .to_string();

        let response_format = match config.get("response_format") {
            Some(Value::String(format)) if format == "openai" => ResponseFormat::OpenAI,
            Some(Value::String(format)) if format == "custom" => {
                let custom_format = config.get("custom_format")
                    .context("Missing 'custom_format' for custom response format")?;
                let content_path = custom_format
                    .get("content_path")
                    .and_then(|v| v.as_str())
                    .context("Missing 'content_path' in custom_format")?
                    .to_string();
                let error_path = custom_format
                    .get("error_path")
                    .and_then(|v| v.as_str())
                    .map(|s| s.to_string());
                ResponseFormat::Custom {
                    content_path,
                    error_path,
                }
            }
            _ => ResponseFormat::OpenAI, // é»˜è®¤ä½¿ç”¨ OpenAI æ ¼å¼
        };

        let timeout = config.get("timeout")
            .and_then(|v| v.as_integer())
            .map(|i| i as u64);

        let retry_count = config.get("retry_count")
            .and_then(|v| v.as_integer())
            .map(|i| i as u32);

        Ok(LLMClientConfig {
            url,
            api_key,
            response_format,
            timeout,
            retry_count,
        })
    }
}
```

---

### æ­¥éª¤ 4ï¼šæ›´æ–°æ¨¡å—å¯¼å‡ºï¼ˆmod.rsï¼‰

**æ–‡ä»¶**ï¼š`src/lib/llm/client/mod.rs`

```rust
//! LLM å®¢æˆ·ç«¯æ¨¡å—
//!
//! æœ¬æ¨¡å—æä¾›äº†ç»Ÿä¸€é…ç½®é©±åŠ¨çš„ LLM å®¢æˆ·ç«¯å®ç°ã€‚

pub mod client;
pub mod common;
pub mod config;

// å‘åå…¼å®¹ï¼šä¿ç•™æ—§çš„å®¢æˆ·ç«¯ï¼ˆæ ‡è®°ä¸º deprecatedï¼‰
#[deprecated(note = "Use client::LLMClient instead")]
pub mod deepseek;
#[deprecated(note = "Use client::LLMClient instead")]
pub mod openai;
#[deprecated(note = "Use client::LLMClient instead")]
pub mod proxy;

pub use common::LLMRequestParams;
pub use config::LLMConfig;
pub use client::{LLMClientConfig, ResponseFormat, LLMClient};
```

---

### æ­¥éª¤ 5ï¼šæ›´æ–° PullRequestLLMï¼ˆpr_llm.rsï¼‰

**å…³é”®æ›´æ”¹**ï¼š

```rust
use super::client::{LLMConfig, LLMRequestParams, LLMClient};

impl PullRequestLLM {
    pub fn generate(
        commit_title: &str,
        exists_branches: Option<Vec<String>>,
        git_diff: Option<String>,
    ) -> Result<PullRequestContent> {
        // 1. å°è¯•ä»é…ç½®æ–‡ä»¶åŠ è½½
        let client = if let Some(config_path) = LLMConfig::find_config_path() {
            match LLMConfig::load(&config_path) {
                Ok(config) => {
                    // ä»é…ç½®æ–‡ä»¶è·å–å®¢æˆ·ç«¯
                    config.get_default_client()?
                }
                Err(e) => {
                    eprintln!("Warning: Failed to load config file: {}", e);
                    // å›é€€åˆ°ç¯å¢ƒå˜é‡
                    LLMConfig::from_env()?
                        .context("No LLM configuration found")?
                }
            }
        } else {
            // ä»ç¯å¢ƒå˜é‡åˆ›å»ºé»˜è®¤é…ç½®
            LLMConfig::from_env()?
                .context("No LLM configuration found")?
        };

        // 2. æ„å»ºè¯·æ±‚å‚æ•°
        let params = LLMRequestParams {
            system_prompt: Self::system_prompt(),
            user_prompt: Self::user_prompt(commit_title, exists_branches, git_diff),
            max_tokens: 100,
            temperature: 0.5,
            model: "gpt-3.5-turbo".to_string(),
        };

        // 3. è°ƒç”¨ç»Ÿä¸€å®¢æˆ·ç«¯
        let response = client.call(&params)?;

        // 4. è§£æå“åº”
        Self::parse_llm_response(response)
    }

    // ... å…¶ä»–æ–¹æ³•ä¿æŒä¸å˜ ...
}
```

---

## ğŸ§ª æµ‹è¯•ç­–ç•¥

### å•å…ƒæµ‹è¯•

```rust
#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_unified_client_openai_format() {
        // æµ‹è¯• OpenAI æ ¼å¼å“åº”è§£æ
    }

    #[test]
    fn test_unified_client_custom_format() {
        // æµ‹è¯•è‡ªå®šä¹‰æ ¼å¼å“åº”è§£æ
    }

    #[test]
    fn test_config_loading() {
        // æµ‹è¯•é…ç½®æ–‡ä»¶åŠ è½½
    }

    #[test]
    fn test_env_var_resolution() {
        // æµ‹è¯•ç¯å¢ƒå˜é‡è§£æ
    }
}
```

### é›†æˆæµ‹è¯•

```rust
#[cfg(test)]
mod integration_tests {
    use super::*;

    #[test]
    #[ignore] // éœ€è¦çœŸå®çš„ API key
    fn test_openai_integration() {
        // æµ‹è¯•çœŸå®çš„ OpenAI API è°ƒç”¨
    }
}
```

---

## ğŸ”„ è¿ç§»è®¡åˆ’

### é˜¶æ®µ 1ï¼šå®ç°æ–°ä»£ç ï¼ˆä¸ç ´åç°æœ‰åŠŸèƒ½ï¼‰

1. âœ… åˆ›å»º `common.rs`ã€`client.rs`ã€`config.rs`
2. âœ… æ›´æ–° `client/mod.rs` å¯¼å‡ºæ–°æ¨¡å—
3. âœ… ä¿æŒ `openai.rs`ã€`deepseek.rs`ã€`proxy.rs` ä¸å˜

### é˜¶æ®µ 2ï¼šæ›´æ–° PullRequestLLMï¼ˆå‘åå…¼å®¹ï¼‰

1. âœ… æ›´æ–° `pr_llm.rs` ä¼˜å…ˆä½¿ç”¨ç»Ÿä¸€å®¢æˆ·ç«¯
2. âœ… å¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå›é€€åˆ°ç¯å¢ƒå˜é‡
3. âœ… å¦‚æœç¯å¢ƒå˜é‡é…ç½®å¤±è´¥ï¼Œå›é€€åˆ°æ—§çš„å®¢æˆ·ç«¯å®ç°

### é˜¶æ®µ 3ï¼šç§»é™¤æ—§ä»£ç ï¼ˆå¯é€‰ï¼‰

1. âš ï¸ æ ‡è®°æ—§å®¢æˆ·ç«¯ä¸º deprecated
2. âš ï¸ ç­‰å¾…ä¸€æ®µæ—¶é—´ç¡®ä¿ç”¨æˆ·è¿ç§»å®Œæˆ
3. âš ï¸ ç§»é™¤ `openai.rs`ã€`deepseek.rs`ã€`proxy.rs`

---

## âœ… å®ç°æ£€æŸ¥æ¸…å•

- [ ] æ·»åŠ  `toml` ä¾èµ–åˆ° `Cargo.toml`
- [ ] åˆ›å»º `src/lib/llm/client/common.rs`
- [ ] åˆ›å»º `src/lib/llm/client/client.rs`
- [ ] åˆ›å»º `src/lib/llm/client/config.rs`
- [ ] æ›´æ–° `src/lib/llm/client/mod.rs`
- [ ] æ›´æ–° `src/lib/llm/pr_llm.rs`
- [ ] æ·»åŠ å•å…ƒæµ‹è¯•
- [ ] æµ‹è¯•é…ç½®æ–‡ä»¶åŠ è½½
- [ ] æµ‹è¯•ç¯å¢ƒå˜é‡å›é€€
- [ ] æµ‹è¯•å‘åå…¼å®¹æ€§

---

## ğŸ“ æ³¨æ„äº‹é¡¹

1. **é”™è¯¯å¤„ç†**ï¼šé…ç½®æ–‡ä»¶ä¸å­˜åœ¨æ—¶åº”è¯¥é™é»˜å›é€€åˆ°ç¯å¢ƒå˜é‡
2. **ç¯å¢ƒå˜é‡è§£æ**ï¼šç¡®ä¿ `${VAR}` å’Œ `${VAR:default}` æ ¼å¼æ­£ç¡®è§£æ
3. **é…ç½®éªŒè¯**ï¼šåŠ è½½é…ç½®åå¿…é¡»éªŒè¯å¿…å¡«å­—æ®µ
4. **å‘åå…¼å®¹**ï¼šä¿æŒç°æœ‰ API ä¸å˜ï¼Œç¡®ä¿ç°æœ‰ä»£ç ç»§ç»­å·¥ä½œ
5. **æµ‹è¯•è¦†ç›–**ï¼šç¡®ä¿æ‰€æœ‰ä»£ç è·¯å¾„éƒ½æœ‰æµ‹è¯•è¦†ç›–

---

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [LLM_PLUGIN_ARCHITECTURE.md](./LLM_PLUGIN_ARCHITECTURE.md) - æ¶æ„è®¾è®¡
- [LLM_PLUGIN_CURL.md](./LLM_PLUGIN_CURL.md) - API è°ƒç”¨ç¤ºä¾‹

