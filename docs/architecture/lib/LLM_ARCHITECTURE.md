# LLM æ¨¡å—æ¶æ„æ–‡æ¡£

## ğŸ“‹ æ¦‚è¿°

LLM æ¨¡å—æ˜¯ Workflow CLI çš„æ ¸å¿ƒåŠŸèƒ½ä¹‹ä¸€ï¼Œæä¾›ç»Ÿä¸€é…ç½®é©±åŠ¨çš„ LLMï¼ˆå¤§è¯­è¨€æ¨¡å‹ï¼‰å®¢æˆ·ç«¯å®ç°ã€‚è¯¥æ¨¡å—é€šè¿‡**ç»Ÿä¸€å®¢æˆ·ç«¯**å’Œ**Settings é…ç½®ç³»ç»Ÿ**ï¼Œå®ç°æ‰€æœ‰ LLM æä¾›å•†çš„ç»Ÿä¸€è°ƒç”¨ï¼Œæ”¯æŒ OpenAIã€DeepSeek å’Œä»£ç† APIã€‚æ‰€æœ‰ LLM æä¾›å•†éƒ½éµå¾ª OpenAI å…¼å®¹æ ¼å¼ï¼Œä½¿ç”¨ç›¸åŒçš„è¯·æ±‚å’Œå“åº”å¤„ç†é€»è¾‘ã€‚

**æ¨¡å—ç»Ÿè®¡ï¼š**
- æ€»ä»£ç è¡Œæ•°ï¼šçº¦ 790 è¡Œ
- æ–‡ä»¶æ•°é‡ï¼š4 ä¸ª
- æ”¯æŒæä¾›å•†ï¼šOpenAIã€DeepSeekã€Proxyï¼ˆä»£ç† APIï¼‰
- ä¸»è¦ç»“æ„ä½“ï¼š`LLMClient`ã€`LLMRequestParams`ã€`PullRequestLLM`ã€`PullRequestContent`

### æ ¸å¿ƒè®¾è®¡åŸåˆ™

1. **ç»Ÿä¸€å®¢æˆ·ç«¯**ï¼šæ‰€æœ‰ LLM æä¾›å•†ä½¿ç”¨åŒä¸€ä¸ªå®¢æˆ·ç«¯å®ç°
2. **é…ç½®é©±åŠ¨**ï¼šæ‰€æœ‰å‚æ•°ï¼ˆURLã€API Keyã€Modelã€Response Formatï¼‰ä» `Settings` åŠ¨æ€è·å–
3. **æ˜“äºæ‰©å±•**ï¼šæ·»åŠ æ–°çš„ LLM æä¾›å•†åªéœ€é…ç½®ï¼Œæ— éœ€å†™ä»£ç 
4. **å‘åå…¼å®¹**ï¼šä¿æŒç°æœ‰ API ä¸å˜ï¼Œæ”¯æŒä» `workflow.toml` é…ç½®

### ä¸ºä»€ä¹ˆé€‰æ‹©ç»Ÿä¸€é…ç½®é©±åŠ¨æ–¹æ¡ˆï¼Ÿ

åŸºäº API è°ƒç”¨åˆ†æï¼Œæ‰€æœ‰ LLM æä¾›å•†éƒ½éµå¾ª **OpenAI å…¼å®¹æ ¼å¼**ï¼š

- âœ… **è¯·æ±‚æ ¼å¼å®Œå…¨ç›¸åŒ**ï¼šéƒ½ä½¿ç”¨ POST åˆ° `/v1/chat/completions` æˆ– `/chat/completions`ï¼Œè¯·æ±‚ä½“ç»“æ„ç›¸åŒ
- âœ… **å“åº”æ ¼å¼å®Œå…¨ç›¸åŒ**ï¼šéƒ½ä» `choices[0].message.content` æå–å†…å®¹ï¼ˆæˆ–é€šè¿‡è‡ªå®šä¹‰ JSON pathï¼‰
- âœ… **å”¯ä¸€å·®å¼‚**ï¼šURL å’Œ API Keyï¼ˆé…ç½®å·®å¼‚ï¼Œéä»£ç å·®å¼‚ï¼‰

**ç»“è®º**ï¼š**ä¸éœ€è¦ä¼ ç»Ÿæ’ä»¶ç³»ç»Ÿ**ï¼ˆtraitã€registryã€managerï¼‰ï¼Œåªéœ€è¦**é…ç½®é©±åŠ¨ + ç»Ÿä¸€å®¢æˆ·ç«¯**æ–¹æ¡ˆã€‚

---

## ğŸ“ æ¨¡å—ç»“æ„

```
src/lib/base/llm/
â”œâ”€â”€ mod.rs          # LLM æ¨¡å—å£°æ˜å’Œå¯¼å‡º (12è¡Œ)
â”œâ”€â”€ client.rs       # LLMClient ç»Ÿä¸€å®¢æˆ·ç«¯ (503è¡Œ)
â””â”€â”€ types.rs        # LLMRequestParams ç±»å‹å®šä¹‰ (34è¡Œ)
```

### ä¸šåŠ¡å±‚å°è£…

```
src/lib/pr/llm.rs   # PullRequestLLM ä¸šåŠ¡å±‚å°è£… (253è¡Œ)
```

**èŒè´£**ï¼š
- æä¾› PR ä¸“ç”¨çš„ LLM æœåŠ¡ï¼ˆç”Ÿæˆåˆ†æ”¯åã€PR æ ‡é¢˜ã€æè¿°ï¼‰
- å°è£… LLM è°ƒç”¨é€»è¾‘ï¼Œæä¾›ä¸šåŠ¡å‹å¥½çš„æ¥å£

### ä¾èµ–æ¨¡å—

- **`lib/base/settings/`**ï¼šé…ç½®ç®¡ç†ï¼ˆä» `workflow.toml` è¯»å– LLM é…ç½®ï¼‰
- **`lib/base/http/`**ï¼šHTTP å“åº”å¤„ç†ï¼ˆ`HttpResponse`ï¼‰
- **`lib/pr/helpers/`**ï¼šPR è¾…åŠ©å‡½æ•°ï¼ˆåˆ†æ”¯åè½¬æ¢ç­‰ï¼‰

### æ¨¡å—é›†æˆ

#### PR æ¨¡å—é›†æˆ

**ä½¿ç”¨åœºæ™¯**ï¼š
- ç”Ÿæˆ PR åˆ†æ”¯å
- ç”Ÿæˆ PR æ ‡é¢˜
- ç”Ÿæˆ PR æè¿°

**å…³é”®è°ƒç”¨**ï¼š
```rust
// é€šè¿‡ PullRequestLLM è°ƒç”¨
let llm = PullRequestLLM::new(commits, config);
let result = llm.generate()?;
```

**ä½ç½®**ï¼š`src/lib/pr/llm.rs`

---

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### è®¾è®¡åŸåˆ™

1. **ç»Ÿä¸€å®¢æˆ·ç«¯**ï¼šæ‰€æœ‰ LLM æä¾›å•†ä½¿ç”¨åŒä¸€ä¸ªå®¢æˆ·ç«¯å®ç°
2. **é…ç½®é©±åŠ¨**ï¼šæ‰€æœ‰å‚æ•°ä» `Settings` åŠ¨æ€è·å–
3. **å•ä¾‹æ¨¡å¼**ï¼šä½¿ç”¨ `OnceLock` å®ç°çº¿ç¨‹å®‰å…¨çš„å…¨å±€å•ä¾‹
4. **æ— çŠ¶æ€è®¾è®¡**ï¼šå®¢æˆ·ç«¯ä¸å­˜å‚¨é…ç½®ï¼Œæ¯æ¬¡è°ƒç”¨æ—¶ä» `Settings` è·å–
5. **ç»Ÿä¸€é”™è¯¯å¤„ç†**ï¼šæ‰€æœ‰æä¾›å•†ä½¿ç”¨ç›¸åŒçš„é”™è¯¯å¤„ç†é€»è¾‘

### æ ¸å¿ƒç»„ä»¶

#### 1. LLMClientï¼ˆç»Ÿä¸€å®¢æˆ·ç«¯ï¼‰

**èŒè´£**ï¼šæä¾›æ‰€æœ‰ LLM æä¾›å•†çš„ç»Ÿä¸€è°ƒç”¨æ¥å£

**ä½ç½®**ï¼š`src/lib/base/llm/client.rs`

**å…³é”®æ–¹æ³•**ï¼š
- `global()` - è·å–å…¨å±€å•ä¾‹
- `call()` - è°ƒç”¨ LLM API
- `build_url()` - æ„å»º API URLï¼ˆæ ¹æ® provider åŠ¨æ€æ„å»ºï¼‰
- `build_headers()` - æ„å»ºè¯·æ±‚å¤´ï¼ˆä» Settings è·å– API Keyï¼‰
- `build_model()` - æ„å»ºæ¨¡å‹åç§°ï¼ˆæ ¹æ® provider è·å–é»˜è®¤å€¼ï¼‰
- `build_payload()` - æ„å»ºè¯·æ±‚ä½“ï¼ˆç»Ÿä¸€æ ¼å¼ï¼‰
- `extract_content()` - æå–å“åº”å†…å®¹ï¼ˆæ”¯æŒæ ‡å‡†æ ¼å¼å’Œè‡ªå®šä¹‰ JSON pathï¼‰
- `extract_by_path()` - é€šè¿‡ JSON path æå–å†…å®¹
- `handle_error()` - ç»Ÿä¸€é”™è¯¯å¤„ç†

**å…³é”®ç‰¹æ€§**ï¼š
- âœ… **å•ä¾‹æ¨¡å¼**ï¼šä½¿ç”¨ `OnceLock` å®ç°çº¿ç¨‹å®‰å…¨çš„å…¨å±€å•ä¾‹
- âœ… **æ— çŠ¶æ€**ï¼šä¸å­˜å‚¨é…ç½®ï¼Œæ¯æ¬¡è°ƒç”¨æ—¶ä» `Settings::get()` è·å–
- âœ… **åŠ¨æ€é…ç½®**ï¼šæ‰€æœ‰é…ç½®ï¼ˆURLã€Keyã€Modelï¼‰éƒ½ä» `Settings` åŠ¨æ€è·å–
- âœ… **ç»Ÿä¸€å¤„ç†**ï¼šæ‰€æœ‰æä¾›å•†ä½¿ç”¨ç›¸åŒçš„è¯·æ±‚å’Œå“åº”å¤„ç†é€»è¾‘
- âœ… **è¶…æ—¶æ§åˆ¶**ï¼š60 ç§’è¶…æ—¶è®¾ç½®
- âœ… **è‡ªå®šä¹‰å“åº”æ ¼å¼**ï¼šæ”¯æŒé€šè¿‡ JSON path æå–å†…å®¹

#### 2. LLMRequestParamsï¼ˆè¯·æ±‚å‚æ•°ï¼‰

**èŒè´£**ï¼šå®šä¹‰ LLM API è¯·æ±‚å‚æ•°

**ä½ç½®**ï¼š`src/lib/base/llm/types.rs`

**å­—æ®µ**ï¼š
- `system_prompt` - ç³»ç»Ÿæç¤ºè¯
- `user_prompt` - ç”¨æˆ·æç¤ºè¯
- `max_tokens` - æœ€å¤§ token æ•°
- `temperature` - æ¸©åº¦å‚æ•°ï¼ˆæ§åˆ¶è¾“å‡ºçš„éšæœºæ€§ï¼‰
- `model` - æ¨¡å‹åç§°ï¼ˆå®é™…ä½¿ç”¨æ—¶ä» Settings è·å–ï¼‰

#### 3. PullRequestLLMï¼ˆä¸šåŠ¡å±‚ï¼‰

**èŒè´£**ï¼šæä¾› PR ä¸“ç”¨çš„ LLM æœåŠ¡

**ä½ç½®**ï¼š`src/lib/pr/llm.rs`

**å…³é”®æ–¹æ³•**ï¼š
- `generate()` - ç”Ÿæˆåˆ†æ”¯åã€PR æ ‡é¢˜å’Œæè¿°
- `system_prompt()` - ç”Ÿæˆç³»ç»Ÿæç¤ºè¯
- `user_prompt()` - ç”Ÿæˆç”¨æˆ·æç¤ºè¯
- `parse_llm_response()` - è§£æ LLM å“åº”

**å…³é”®ç‰¹æ€§**ï¼š
- âœ… **ä¸šåŠ¡å°è£…**ï¼šå°è£… LLM è°ƒç”¨é€»è¾‘ï¼Œæä¾›ä¸šåŠ¡å‹å¥½çš„æ¥å£
- âœ… **æ™ºèƒ½ç”Ÿæˆ**ï¼šæ ¹æ® commit æ ‡é¢˜å’Œ Git diff ç”Ÿæˆåˆ†æ”¯åå’Œ PR æ ‡é¢˜
- âœ… **å¤šè¯­è¨€æ”¯æŒ**ï¼šè‡ªåŠ¨ç¿»è¯‘éè‹±æ–‡å†…å®¹ä¸ºè‹±æ–‡
- âœ… **å“åº”è§£æ**ï¼šæ”¯æŒ JSON å’Œ Markdown ä»£ç å—æ ¼å¼

#### 4. PullRequestContentï¼ˆä¸šåŠ¡æ•°æ®ï¼‰

**èŒè´£**ï¼šå®šä¹‰ PR å†…å®¹ç»“æ„

**ä½ç½®**ï¼š`src/lib/pr/llm.rs`

**å­—æ®µ**ï¼š
- `branch_name` - åˆ†æ”¯åç§°ï¼ˆå°å†™ï¼Œä½¿ç”¨è¿å­—ç¬¦åˆ†éš”ï¼‰
- `pr_title` - PR æ ‡é¢˜ï¼ˆç®€æ´ï¼Œä¸è¶…è¿‡ 8 ä¸ªå•è¯ï¼‰
- `description` - PR æè¿°ï¼ˆåŸºäº Git ä¿®æ”¹å†…å®¹ç”Ÿæˆï¼Œå¯é€‰ï¼‰

#### 5. Settingsï¼ˆé…ç½®ç³»ç»Ÿï¼‰

**èŒè´£**ï¼šä» `workflow.toml` è¯»å– LLM é…ç½®

**é…ç½®ä½ç½®**ï¼š`workflow.toml` æ–‡ä»¶çš„ `[llm]` éƒ¨åˆ†

**é…ç½®å­—æ®µ**ï¼š

| å­—æ®µ | ç±»å‹ | å¿…éœ€ | è¯´æ˜ |
|------|------|------|------|
| `provider` | String | âœ… | æä¾›å•†åç§°ï¼š`openai`ã€`deepseek`ã€`proxy` |
| `key` | String | âœ… | API Key |
| `url` | String | âš ï¸ | API URLï¼ˆä»… `proxy` æä¾›å•†éœ€è¦ï¼‰ |
| `model` | String | âš ï¸ | æ¨¡å‹åç§°ï¼ˆ`openai`/`deepseek` æœ‰é»˜è®¤å€¼ï¼Œ`proxy` å¿…å¡«ï¼‰ |
| `response_format` | String | âŒ | å“åº”æ ¼å¼è·¯å¾„ï¼ˆé»˜è®¤ï¼š`choices[0].message.content`ï¼‰ |

**é»˜è®¤å€¼**ï¼š
- `provider`: `"openai"`
- `model`:
  - `openai`: `"gpt-4.0"`
  - `deepseek`: `"deepseek-chat"`
  - `proxy`: æ— é»˜è®¤å€¼ï¼Œå¿…é¡»é…ç½®
- `response_format`: `"choices[0].message.content"`

### è®¾è®¡æ¨¡å¼

#### 1. å•ä¾‹æ¨¡å¼

é€šè¿‡ `OnceLock` å®ç°çº¿ç¨‹å®‰å…¨çš„å…¨å±€å•ä¾‹ï¼š

```rust
pub fn global() -> &'static Self {
    static CLIENT: OnceLock<Self> = OnceLock::new();
    CLIENT.get_or_init(Self::new)
}
```

**ä¼˜åŠ¿**ï¼š
- å®¢æˆ·ç«¯åªåˆå§‹åŒ–ä¸€æ¬¡ï¼Œæé«˜æ€§èƒ½
- çº¿ç¨‹å®‰å…¨ï¼Œå¯ä»¥åœ¨å¤šçº¿ç¨‹ç¯å¢ƒä¸­å®‰å…¨ä½¿ç”¨
- ç»Ÿä¸€ç®¡ç†ï¼Œæ‰€æœ‰æ¨¡å—ä½¿ç”¨åŒä¸€ä¸ªå®¢æˆ·ç«¯å®ä¾‹

#### 2. é…ç½®é©±åŠ¨æ¨¡å¼

æ‰€æœ‰é…ç½®ä» `Settings` åŠ¨æ€è·å–ï¼Œæ— éœ€ç¡¬ç¼–ç ï¼š

```rust
let settings = Settings::get();
let url = self.build_url(&settings.llm.provider, &settings.llm.url)?;
let model = self.build_model(&settings.llm)?;
```

**ä¼˜åŠ¿**ï¼š
- æ˜“äºé…ç½®ï¼Œæ— éœ€ä¿®æ”¹ä»£ç 
- æ”¯æŒè¿è¡Œæ—¶åˆ‡æ¢æä¾›å•†
- é…ç½®é›†ä¸­ç®¡ç†

#### 3. ç»Ÿä¸€å®¢æˆ·ç«¯æ¨¡å¼

æ‰€æœ‰ LLM æä¾›å•†ä½¿ç”¨åŒä¸€ä¸ªå®¢æˆ·ç«¯å®ç°ï¼š

```rust
impl LLMClient {
    pub fn call(&self, params: &LLMRequestParams) -> Result<String> {
        // ç»Ÿä¸€çš„è¯·æ±‚å’Œå“åº”å¤„ç†é€»è¾‘
    }
}
```

**ä¼˜åŠ¿**ï¼š
- ä»£ç å¤ç”¨ï¼Œå‡å°‘é‡å¤
- æ˜“äºç»´æŠ¤ï¼Œä¿®æ”¹ä¸€å¤„å³å¯
- ç»Ÿä¸€çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•

#### 4. ç­–ç•¥æ¨¡å¼ï¼ˆéšå¼ï¼‰

é€šè¿‡é…ç½®é€‰æ‹©ä¸åŒçš„ LLM æä¾›å•†ï¼š

```rust
match provider.as_str() {
    "openai" => "https://api.openai.com/v1/chat/completions",
    "deepseek" => "https://api.deepseek.com/chat/completions",
    "proxy" => format!("{}/chat/completions", url),
    _ => bail!("Unsupported LLM provider: {}", provider),
}
```

**ä¼˜åŠ¿**ï¼š
- æ˜“äºæ‰©å±•ï¼Œæ·»åŠ æ–°æä¾›å•†åªéœ€é…ç½®
- æ— éœ€ä¿®æ”¹ä»£ç ï¼Œåªéœ€é…ç½®æ–‡ä»¶

### é”™è¯¯å¤„ç†

#### åˆ†å±‚é”™è¯¯å¤„ç†

1. **é…ç½®é”™è¯¯**ï¼š
   - Provider ä¸æ”¯æŒï¼šè¿”å›é”™è¯¯
   - API Key æœªé…ç½®ï¼šè¿”å›é”™è¯¯
   - URL æœªé…ç½®ï¼ˆproxy æä¾›å•†ï¼‰ï¼šè¿”å›é”™è¯¯

2. **ç½‘ç»œé”™è¯¯**ï¼š
   - è¿æ¥å¤±è´¥ï¼šè¿”å›é”™è¯¯
   - è¶…æ—¶ï¼ˆ60 ç§’ï¼‰ï¼šè¿”å›é”™è¯¯

3. **API é”™è¯¯**ï¼š
   - HTTP çŠ¶æ€ç é 200ï¼šè¿”å›é”™è¯¯
   - å“åº”æ ¼å¼é”™è¯¯ï¼šè¿”å›é”™è¯¯

4. **è§£æé”™è¯¯**ï¼š
   - JSON è§£æå¤±è´¥ï¼šè¿”å›é”™è¯¯
   - å“åº”å†…å®¹æå–å¤±è´¥ï¼šè¿”å›é”™è¯¯

#### å®¹é”™æœºåˆ¶

- **é…ç½®éªŒè¯**ï¼šåœ¨è°ƒç”¨å‰éªŒè¯é…ç½®æ˜¯å¦å®Œæ•´
- **è¶…æ—¶æ§åˆ¶**ï¼š60 ç§’è¶…æ—¶è®¾ç½®ï¼Œé¿å…é•¿æ—¶é—´ç­‰å¾…
- **è¯¦ç»†é”™è¯¯ä¿¡æ¯**ï¼šæä¾›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ï¼ŒåŒ…å« HTTP çŠ¶æ€ç å’Œå“åº”ä½“
- **ç»Ÿä¸€é”™è¯¯å¤„ç†**ï¼šæ‰€æœ‰æä¾›å•†ä½¿ç”¨ç›¸åŒçš„é”™è¯¯å¤„ç†é€»è¾‘

---

## ğŸ”„ è°ƒç”¨æµç¨‹ä¸æ•°æ®æµ

### æ•´ä½“æ¶æ„æµç¨‹

```
ç”¨æˆ·è¾“å…¥ï¼ˆcommit æ ‡é¢˜ã€Git diffï¼‰
  â†“
PullRequestLLM::generate()
  â†“
LLMClient::global() (è·å–å…¨å±€å•ä¾‹)
  â†“
LLMClient::call() (è°ƒç”¨ LLM API)
  â”œâ”€ build_url() (ä» Settings è·å– URL)
  â”œâ”€ build_headers() (ä» Settings è·å– API Key)
  â”œâ”€ build_model() (ä» Settings è·å– Model)
  â”œâ”€ build_payload() (æ„å»ºè¯·æ±‚ä½“)
  â””â”€ extract_content() (æå–å“åº”å†…å®¹)
  â†“
reqwest HTTP Client (å‘é€è¯·æ±‚)
  â†“
LLM API (OpenAI/DeepSeek/Proxy)
  â†“
è§£æå“åº”å¹¶è¿”å›
```

#### æ¶æ„æµç¨‹å›¾

```mermaid
graph TB
    User[ç”¨æˆ·è¾“å…¥<br/>commit æ ‡é¢˜<br/>Git diff] --> PRLLM[PullRequestLLM::generate<br/>ä¸šåŠ¡å±‚å°è£…]

    PRLLM --> Client[LLMClient::global<br/>è·å–å…¨å±€å•ä¾‹]

    Client --> BuildURL[build_url<br/>æ„å»º API URL]
    Client --> BuildHeaders[build_headers<br/>æ„å»ºè¯·æ±‚å¤´]
    Client --> BuildModel[build_model<br/>è·å–æ¨¡å‹åç§°]
    Client --> BuildPayload[build_payload<br/>æ„å»ºè¯·æ±‚ä½“]

    BuildURL --> Settings[Settings::get<br/>è¯»å–é…ç½®]
    BuildHeaders --> Settings
    BuildModel --> Settings

    Settings --> Config[workflow.toml<br/>é…ç½®æ–‡ä»¶]

    BuildPayload --> HTTP[reqwest HTTP Client<br/>å‘é€è¯·æ±‚]

    HTTP --> Provider{LLM Provider}

    Provider -->|openai| OpenAI[OpenAI API<br/>https://api.openai.com]
    Provider -->|deepseek| DeepSeek[DeepSeek API<br/>https://api.deepseek.com]
    Provider -->|proxy| Proxy[Proxy API<br/>è‡ªå®šä¹‰ URL]

    OpenAI --> Response[è§£æå“åº”]
    DeepSeek --> Response
    Proxy --> Response

    Response --> Extract[extract_content<br/>æå–å†…å®¹]
    Extract --> Parse[parse_llm_response<br/>è§£æ JSON]
    Parse --> Result[PullRequestContent<br/>è¿”å›ç»“æœ]

    style User fill:#e1f5ff
    style PRLLM fill:#fff4e1
    style Client fill:#e8f5e9
    style Settings fill:#f3e5f5
    style Config fill:#f3e5f5
    style HTTP fill:#e3f2fd
    style OpenAI fill:#e3f2fd
    style DeepSeek fill:#e3f2fd
    style Proxy fill:#e3f2fd
    style Result fill:#c8e6c9
```

### å…¸å‹è°ƒç”¨ç¤ºä¾‹

#### 1. ç”Ÿæˆ PR æ ‡é¢˜å’Œåˆ†æ”¯å

```
PullRequestLLM::generate(commit_title, exists_branches, git_diff)
  â†“
LLMClient::global() (è·å–å…¨å±€å•ä¾‹)
  â†“
LLMClient::call(&params)
  â”œâ”€ build_url() â†’ Settings::get().llm.provider
  â”‚   â”œâ”€ "openai" â†’ "https://api.openai.com/v1/chat/completions"
  â”‚   â”œâ”€ "deepseek" â†’ "https://api.deepseek.com/chat/completions"
  â”‚   â””â”€ "proxy" â†’ Settings::get().llm.url + "/chat/completions"
  â”œâ”€ build_headers() â†’ Settings::get().llm.key
  â”œâ”€ build_model() â†’ Settings::get().llm.model (æˆ–é»˜è®¤å€¼)
  â”œâ”€ build_payload() â†’ æ„å»ºç»Ÿä¸€æ ¼å¼çš„è¯·æ±‚ä½“
  â””â”€ extract_content() â†’ æ ¹æ® response_format æå–å†…å®¹
  â†“
reqwest::Client::post() (å‘é€ HTTP è¯·æ±‚)
  â†“
LLM API å“åº”
  â†“
parse_llm_response() â†’ PullRequestContent
```

### æ•°æ®æµ

#### ç”Ÿæˆ PR æ ‡é¢˜å’Œåˆ†æ”¯åæ•°æ®æµ

```mermaid
flowchart LR
    Input[ç”¨æˆ·è¾“å…¥<br/>commit æ ‡é¢˜<br/>Git diff] --> PRLLM[PullRequestLLM::generate]

    PRLLM --> BuildParams[æ„å»ºè¯·æ±‚å‚æ•°<br/>system_prompt<br/>user_prompt]

    BuildParams --> Client[LLMClient::global<br/>è·å–å•ä¾‹]

    Client --> ReadConfig[è¯»å–é…ç½®<br/>Settings::get]

    ReadConfig --> Config[workflow.toml<br/>llm.provider<br/>llm.key<br/>llm.model]

    Config --> BuildRequest[æ„å»ºè¯·æ±‚<br/>URL/Headers/Payload]

    BuildRequest --> HTTP[å‘é€ HTTP è¯·æ±‚<br/>reqwest]

    HTTP --> Provider{LLM Provider}

    Provider -->|openai| OpenAI[OpenAI API]
    Provider -->|deepseek| DeepSeek[DeepSeek API]
    Provider -->|proxy| Proxy[Proxy API]

    OpenAI --> Response[HTTP å“åº”]
    DeepSeek --> Response
    Proxy --> Response

    Response --> ParseJSON[è§£æ JSON]
    ParseJSON --> Extract[æå–å†…å®¹<br/>extract_content]
    Extract --> ParseResponse[è§£æä¸šåŠ¡æ•°æ®<br/>parse_llm_response]
    ParseResponse --> Result[PullRequestContent<br/>branch_name<br/>pr_title<br/>description]

    style Input fill:#e1f5ff
    style PRLLM fill:#fff4e1
    style Client fill:#e8f5e9
    style Config fill:#f3e5f5
    style HTTP fill:#e3f2fd
    style OpenAI fill:#e3f2fd
    style DeepSeek fill:#e3f2fd
    style Proxy fill:#e3f2fd
    style Result fill:#c8e6c9
```

---

## ğŸ“ æ‰©å±•æ€§

### æ·»åŠ æ–°çš„ LLM æä¾›å•†

1. åœ¨ `Settings` ä¸­æ·»åŠ æ–°çš„ provider åç§°
2. åœ¨ `LLMClient::build_url()` ä¸­æ·»åŠ æ–° provider çš„ URL æ„å»ºé€»è¾‘
3. åœ¨ `LLMClient::build_model()` ä¸­æ·»åŠ æ–° provider çš„é»˜è®¤æ¨¡å‹ï¼ˆå¦‚éœ€è¦ï¼‰
4. åœ¨ `workflow.toml` ä¸­é…ç½®æ–° provider çš„ URL å’Œ API Key

**ç¤ºä¾‹**ï¼š
```rust
// lib/base/llm/client.rs
fn build_url(&self) -> Result<String> {
    let settings = Settings::get();
    match settings.llm.provider.as_str() {
        "openai" => Ok("https://api.openai.com/v1/chat/completions".to_string()),
        "deepseek" => Ok("https://api.deepseek.com/chat/completions".to_string()),
        "proxy" => Ok(format!("{}/chat/completions", settings.llm.url?)),
        "new_provider" => Ok("https://api.newprovider.com/chat/completions".to_string()), // æ–°å¢
        _ => Err(anyhow::anyhow!("Unsupported LLM provider: {}", provider)),
    }
}
```

### æ·»åŠ æ–°çš„ä¸šåŠ¡åŠŸèƒ½

1. åœ¨ `lib/pr/llm.rs` æˆ–æ–°å»ºä¸šåŠ¡æ¨¡å—ä¸­æ·»åŠ æ–°çš„ä¸šåŠ¡æ–¹æ³•
2. ä½¿ç”¨ `LLMClient::global()` è°ƒç”¨ LLM API
3. å®ç°ä¸šåŠ¡ç‰¹å®šçš„ prompt æ„å»ºå’Œå“åº”è§£æé€»è¾‘

### è‡ªå®šä¹‰å“åº”æ ¼å¼

é€šè¿‡ `response_format` é…ç½®æ”¯æŒè‡ªå®šä¹‰ JSON pathï¼š

```toml
[llm]
provider = "proxy"
url = "https://api.example.com"
key = "your-api-key"
model = "custom-model"
response_format = "candidates[0].content.parts[0].text"  # è‡ªå®šä¹‰è·¯å¾„
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [ä¸»æ¶æ„æ–‡æ¡£](../ARCHITECTURE.md)
- [PR æ¨¡å—æ¶æ„æ–‡æ¡£](./PR_ARCHITECTURE.md) - PR æ¨¡å—å¦‚ä½•ä½¿ç”¨ LLM åŠŸèƒ½
- [Settings æ¨¡å—æ¶æ„æ–‡æ¡£](./SETTINGS_ARCHITECTURE.md) - Settings é…ç½®ç³»ç»Ÿæ¶æ„

---

## ğŸ“‹ ä½¿ç”¨ç¤ºä¾‹

### åŸºæœ¬ä½¿ç”¨ï¼ˆä¸šåŠ¡å±‚ï¼‰

```rust
use workflow::pr::PullRequestLLM;

// ç”Ÿæˆåˆ†æ”¯åå’Œ PR æ ‡é¢˜
let content = PullRequestLLM::generate(
    "Fix login bug",
    Some(vec!["feature-1".to_string(), "feature-2".to_string()]),
    Some(git_diff),
)?;

println!("Branch: {}", content.branch_name);
println!("PR Title: {}", content.pr_title);
if let Some(desc) = content.description {
    println!("Description: {}", desc);
}
```

### ç›´æ¥ä½¿ç”¨ LLMClient

```rust
use workflow::base::llm::{LLMClient, LLMRequestParams};

let client = LLMClient::global();

let params = LLMRequestParams {
    system_prompt: "You are a helpful assistant.".to_string(),
    user_prompt: "What is Rust?".to_string(),
    max_tokens: 100,
    temperature: 0.5,
    model: String::new(), // ä» Settings è‡ªåŠ¨è·å–
};

let response = client.call(&params)?;
println!("{}", response);
```

### é…ç½® OpenAI

```toml
# workflow.toml
[llm]
provider = "openai"
key = "sk-xxx"
model = "gpt-4.0"  # å¯é€‰ï¼Œé»˜è®¤ "gpt-4.0"
```

### é…ç½® DeepSeek

```toml
# workflow.toml
[llm]
provider = "deepseek"
key = "sk-xxx"
model = "deepseek-chat"  # å¯é€‰ï¼Œé»˜è®¤ "deepseek-chat"
```

### é…ç½® Proxyï¼ˆä»£ç† APIï¼‰

```toml
# workflow.toml
[llm]
provider = "proxy"
url = "https://proxy.example.com"  # å¿…éœ€
key = "your-api-key"                # å¿…éœ€
model = "qwen-3-235b"               # å¿…éœ€
response_format = "choices[0].message.content"  # å¯é€‰ï¼Œé»˜è®¤å€¼
```

### è‡ªå®šä¹‰å“åº”æ ¼å¼

```toml
# workflow.toml
[llm]
provider = "proxy"
url = "https://api.example.com"
key = "your-api-key"
model = "custom-model"
response_format = "candidates[0].content.parts[0].text"  # è‡ªå®šä¹‰ JSON path
```

---

## âœ… æ€»ç»“

LLM æ¨¡å—é‡‡ç”¨ç»Ÿä¸€é…ç½®é©±åŠ¨æ¶æ„è®¾è®¡ï¼š

1. **ç»Ÿä¸€å®¢æˆ·ç«¯**ï¼šæ‰€æœ‰ LLM æä¾›å•†ä½¿ç”¨åŒä¸€ä¸ª `LLMClient` å®ç°
2. **é…ç½®é©±åŠ¨**ï¼šæ‰€æœ‰å‚æ•°ä» `Settings` åŠ¨æ€è·å–ï¼Œæ”¯æŒé€šè¿‡ `workflow.toml` é…ç½®
3. **å•ä¾‹æ¨¡å¼**ï¼šä½¿ç”¨ `OnceLock` å®ç°çº¿ç¨‹å®‰å…¨çš„å…¨å±€å•ä¾‹
4. **ä¸šåŠ¡å°è£…**ï¼š`PullRequestLLM` æä¾›ä¸šåŠ¡å‹å¥½çš„æ¥å£
5. **çµæ´»æ‰©å±•**ï¼šæ·»åŠ æ–°æä¾›å•†åªéœ€é…ç½®ï¼Œæ— éœ€å†™ä»£ç 

**è®¾è®¡ä¼˜åŠ¿**ï¼š
- âœ… **ä»£ç å¤ç”¨**ï¼šæ¶ˆé™¤ä»£ç é‡å¤ï¼Œä»å¤šä¸ªç‹¬ç«‹å®¢æˆ·ç«¯å‡å°‘åˆ°ä¸€ä¸ªç»Ÿä¸€å®¢æˆ·ç«¯
- âœ… **æ˜“äºæ‰©å±•**ï¼šæ·»åŠ æ–°æä¾›å•†åªéœ€é…ç½®ï¼Œæ— éœ€å†™ä»£ç 
- âœ… **ç»Ÿä¸€ç®¡ç†**ï¼šæ‰€æœ‰é…ç½®é›†ä¸­åœ¨ `workflow.toml`
- âœ… **çµæ´»é…ç½®**ï¼šæ”¯æŒè‡ªå®šä¹‰å“åº”æ ¼å¼ï¼ˆJSON pathï¼‰
- âœ… **å‘åå…¼å®¹**ï¼šä¿æŒç°æœ‰ API ä¸å˜
- âœ… **ç»´æŠ¤æˆæœ¬ä½**ï¼šåªéœ€ç»´æŠ¤ä¸€ä¸ªç»Ÿä¸€å®¢æˆ·ç«¯

**å½“å‰å®ç°çŠ¶æ€**ï¼š

âœ… **å·²å®ç°**ï¼š
- ç»Ÿä¸€ `LLMClient` å®ç°
- åŸºäº `Settings` çš„é…ç½®ç³»ç»Ÿ
- æ”¯æŒ OpenAIã€DeepSeekã€Proxy æä¾›å•†
- è‡ªå®šä¹‰å“åº”æ ¼å¼æ”¯æŒï¼ˆJSON pathï¼‰
- PR ä¸šåŠ¡å±‚å°è£…ï¼ˆ`PullRequestLLM`ï¼‰
- å•ä¾‹æ¨¡å¼å®ç°

**é…ç½®è¯´æ˜**ï¼š
- å½“å‰å®ç°ä½¿ç”¨ `workflow.toml` çš„ `[llm]` éƒ¨åˆ†è¿›è¡Œé…ç½®
- æ‰€æœ‰ LLM ç›¸å…³é…ç½®ç»Ÿä¸€å­˜å‚¨åœ¨ `workflow.toml` ä¸­ï¼Œä¸é¡¹ç›®é…ç½®ç»Ÿä¸€ç®¡ç†
- é€šè¿‡ `workflow setup` å‘½ä»¤å¯ä»¥äº¤äº’å¼é…ç½® LLM æä¾›å•†
